
@inproceedings{hoellt_semantic_2023,
	title = {Semantic {Hierarchical} {Exploration} of {Large} {Image} {Datasets}},
	isbn = {978-3-03868-219-6},
	doi = {10.2312/evs.20231051},
	booktitle = {{EuroVis} 2023 - {Short} {Papers}},
	publisher = {The Eurographics Association},
	author = {Bäuerle, Alex and Onzenoodt, Christian van and Jönsson, Daniel and Ropinski, Timo},
	editor = {Hoellt, Thomas and Aigner, Wolfgang and Wang, Bei},
	year = {2023},
	file = {Full Text:C\:\\Users\\Richard\\Zotero\\storage\\C6JFV6D8\\Bäuerle et al. - 2023 - Semantic Hierarchical Exploration of Large Image D.pdf:application/pdf},
}

@article{bertucci_dendromap_2023,
	title = {{DendroMap}: {Visual} {Exploration} of {Large}-{Scale} {Image} {Datasets} for {Machine} {Learning} with {Treemaps}},
	volume = {29},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2022.3209425},
	abstract = {In this paper, we present DendroMap, a novel approach to interactively exploring large-scale image datasets for machine learning (ML). ML practitioners often explore image datasets by generating a grid of images or projecting high-dimensional representations of images into 2-D using dimensionality reduction techniques (e.g., t-SNE). However, neither approach effectively scales to large datasets because images are ineffectively organized and interactions are insufficiently supported. To address these challenges, we develop DendroMap by adapting Treemaps, a well-known visualization technique. DendroMap effectively organizes images by extracting hierarchical cluster structures from high-dimensional representations of images. It enables users to make sense of the overall distributions of datasets and interactively zoom into specific areas of interests at multiple levels of abstraction. Our case studies with widely-used image datasets for deep learning demonstrate that users can discover insights about datasets and trained models by examining the diversity of images, identifying underperforming subgroups, and analyzing classification errors. We conducted a user study that evaluates the effectiveness of DendroMap in grouping and searching tasks by comparing it with a gridified version of t-SNE and found that participants preferred DendroMap. DendroMap is available at https://div-lab.github.io/dendromap/.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Bertucci, Donald and Hamid, Md Montaser and Anand, Yashwanthi and Ruangrotsakun, Anita and Tabatabai, Delyar and Perez, Melissa and Kahng, Minsuk},
	year = {2023},
	pages = {320--330},
	file = {Bertucci et al. - 2023 - DendroMap Visual Exploration of Large-Scale Image.pdf:C\:\\Users\\Richard\\Zotero\\storage\\RFDW9DXB\\Bertucci et al. - 2023 - DendroMap Visual Exploration of Large-Scale Image.pdf:application/pdf},
}

@inproceedings{sharma_image_2015,
	title = {Image summarization using topic modelling},
	doi = {10.1109/ICSIPA.2015.7412194},
	abstract = {With the rapid advancement in technology it is easy to take pictures, and easier still to share them with the world at large. In this digital age the image data we have has grown manifold over the past few years. Digital Cameras have allowed the capturing and storing of large number of images highly inexpensive and hence we can't go on a trip without taking several hundreds of photographs. We take repeated shots till we find the perfect one. As a result, in a collection of images we end up having recurring and similar images which we would like to omit. Manually going through such a huge collection and picking out the best images is a highly laborious task. This paper deals with devising a way of summarizing a given collection of photographs to represent a distinct set of representative images. This would save the users a lot of effort while allowing the selection of the best images of the set which also represent the entire set. Here we employ a modified Latent Dirichlet Allocation technique, a generative probabilistic model, to partition the images from a ‘Bag of words’ representation created using Scale Invariant Feature Transform (SIFT) vectors and then clustering these vectors into bins. We validate the results using subjective analysis based on 3 metrics by the people providing the image collection and also by a more general set of people.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Signal} and {Image} {Processing} {Applications} ({ICSIPA})},
	author = {Sharma, Vasu and Kumar, Akshay and Agrawal, Nishant and Singh, Puneet and Kulshreshtha, Rajat},
	month = oct,
	year = {2015},
	pages = {226--231},
	file = {Sharma et al. - 2015 - Image summarization using topic modelling.pdf:C\:\\Users\\Richard\\Zotero\\storage\\NZTUEBPW\\Sharma et al. - 2015 - Image summarization using topic modelling.pdf:application/pdf},
}

@inproceedings{miranda_urban_2020,
	title = {Urban {Mosaic}: {Visual} {Exploration} of {Streetscapes} {Using} {Large}-{Scale} {Image} {Data}},
	isbn = {978-1-4503-6708-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091280396&doi=10.1145%2f3313831.3376399&partnerID=40&md5=7be6991fc5fa18310bfb50cf83a46862},
	doi = {10.1145/3313831.3376399},
	abstract = {Urban planning is increasingly data driven, yet the challenge of designing with data at a city scale and remaining sensitive to the impact at a human scale is as important today as it was for Jane Jacobs. We address this challenge with Urban Mosaic, a tool for exploring the urban fabric through a spatially and temporally dense data set of 7.7 million street-level images from New York City, captured over the period of a year. Working in collaboration with professional practitioners, we use Urban Mosaic to investigate questions of accessibility and mobility, and preservation and retrofitting. In doing so, we demonstrate how tools such as this might provide a bridge between the city and the street, by supporting activities such as visual comparison of geographically distant neighborhoods, and temporal analysis of unfolding urban development. © 2020 ACM.},
	language = {English},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Miranda, Fabio and Hosseini, Maryam and Lage, Marcos and Doraiswamy, Harish and Dove, Graham and Silva, Cláudio T.},
	year = {2020},
	note = {Type: Conference paper},
	keywords = {Visual comparison, Temporal analysis, Data driven, Dense datum, Human engineering, New York city, Urban development, Urban fabrics, Urban growth, Visual exploration},
	file = {Miranda et al. - 2020 - Urban Mosaic Visual Exploration of Streetscapes U.pdf:C\:\\Users\\Richard\\Zotero\\storage\\86SR7BUJ\\Miranda et al. - 2020 - Urban Mosaic Visual Exploration of Streetscapes U.pdf:application/pdf},
}

@article{zhang_joint_2019,
	title = {Joint optimisation convex-negative matrix factorisation for multi-modal image collection summarisation based on images and tags},
	volume = {13},
	issn = {17519632},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062046434&doi=10.1049%2fiet-cvi.2017.0568&partnerID=40&md5=98a4283061137b435c593070aa96219c},
	doi = {10.1049/iet-cvi.2017.0568},
	abstract = {mage collection summarisation aims to represent a large-scale multi-modal collection with a small subset of images and tags, helping navigate a large image dataset. Most extant methods leverage the contributions of text-to-visual summaries, ignoring the visual contribution to the textual topic. When the tags are weakly labelled, the textual topic cannot accurately reflect the visual summary. To solve this, the authors propose a novel model, joint optimisation of convex non-negative matrix factorisation, which incorporates images and tags in a beneficial way. The objective function contains visual and textual error functions, sharing the same indicator matrix, connecting different modal relations. Then, they propose an iterative algorithm to optimise the proposed model. Finally, they explore the effects of different visual feature representations (e.g. bag-of-words and deep learning) on multi-modal collection summary. Our proposed method is then compared with state-of-the-art algorithms using two multi-modal datasets (i.e. MIRFlickr and NUS-WIDE-SCENE). Experimental results demonstrate the effectiveness of their proposed approach. © The Institution of Engineering and Technology 2018.},
	language = {English},
	number = {2},
	journal = {IET Computer Vision},
	author = {Zhang, Wenkai and Fu, Kun and Sun, Xian and Zhang, Yuhang and Sun, Hao and Wang, Hongqi},
	year = {2019},
	note = {Publisher: Institution of Engineering and Technology
Type: Article},
	keywords = {Deep learning, Large dataset, Factorization, Indicator matrix, Iterative algorithm, Iterative methods, Matrix factorisation, Multi-modal image, Non-negative matrix factorisation, Objective functions, State-of-the-art algorithms, Visual contributions},
	pages = {125 -- 130},
	file = {Zhang et al. - 2019 - Joint optimisation convex-negative matrix factoris.pdf:C\:\\Users\\Richard\\Zotero\\storage\\9YQ3IVXH\\Zhang et al. - 2019 - Joint optimisation convex-negative matrix factoris.pdf:application/pdf},
}

@inproceedings{chang_style-centric_2016,
	title = {Style-centric image summarization from photographic views of a city},
	volume = {2016-May},
	isbn = {978-1-4799-9988-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973315946&doi=10.1109%2fICASSP.2016.7472185&partnerID=40&md5=b5ef38843187b5e5979515e762f9fa57},
	doi = {10.1109/ICASSP.2016.7472185},
	abstract = {Visual summarization addresses the task of selecting images from an image collection, so that the sampled images would contain representative information which sufficiently highlights the collected visual data. In this paper, we solve the problem of style-centric visual summarization using photographic landmark images of a city. Different from existing works which typically retrieve landmark images based on salient visual appearances, our proposed method is able to produce different sets of summarized images, while each set corresponds to a particular image style. This is achieved by performing unsupervised clustering on images within and across landmark categories, which discovers the common photographic styles from the input image collection. Our experiments will confirm that, compared to standard clustering algorithms, our approach is able to achieve satisfactory summarization outputs with style consistency. © 2016 IEEE.},
	language = {English},
	booktitle = {{ICASSP}, {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Chang, Wei-Yi and Wang, Yu-Chiang Frank},
	year = {2016},
	note = {ISSN: 15206149
Type: Conference paper},
	pages = {2787 -- 2791},
	file = {Chang and Wang - 2016 - Style-centric image summarization from photographi.pdf:C\:\\Users\\Richard\\Zotero\\storage\\CGK732NX\\Chang and Wang - 2016 - Style-centric image summarization from photographi.pdf:application/pdf},
}

@article{rayar_viewable_2018,
	title = {A viewable indexing structure for the interactive exploration of dynamic and large image collections},
	volume = {12},
	issn = {15564681},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042523112&doi=10.1145%2f3047011&partnerID=40&md5=c165eaf1f739055533ac7f23454e07e2},
	doi = {10.1145/3047011},
	abstract = {Thanks to the capturing devices cost reduction and the advent of social networks, the size of image collections is becoming extremely huge. Many works in the literature have addressed the indexing of large image collections for search purposes. However, there is a lack of support for exploratory data mining. One May want to wander around the images and experience serendipity in the exploration process. Thus, effective paradigms not only for organising, but also visualising these image collections become necessary. In this article, we present a study to jointly index and visualise large image collections. The work focuses on satisfying three constraints. First, large image collections, up to million of images, shall be handled. Second, dynamic collections, such as ever-growing collections, shall be processed in an incremental way, without reprocessing the whole collection at each modification. Finally, an intuitive and interactive exploration system shall be provided to the user to allow him to easily mine image collections. To this end, a data partitioning algorithm has been modified and proximity graphs have been used to fit the visualisation purpose. A custom web platform has been implemented to visualise the hierarchical and graph-based hybrid structure. The results of a user evaluation we have conducted show that the exploration of the collections is intuitive and smooth thanks to the proposed structure. Furthermore, the scalability of the proposed indexing method is proved using large public image collections. © 2018 ACM.},
	language = {English},
	number = {1},
	journal = {ACM Transactions on Knowledge Discovery from Data},
	author = {Rayar, Frédéric and Barrat, Sabine and Bouali, Fatma and Venturini, Gilles},
	year = {2018},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Visualization, Data mining, Graphic methods, Interactive visualisation, Large images, Cost reduction, Data partitioning algorithms, Exploration process, Exploratory data mining, Incremental construction, Indexing (of information), Indexing structures, Interactive exploration},
	file = {Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\37JJH3QD\\Rayar et al. - 2018 - A Viewable Indexing Structure for the Interactive .pdf:application/pdf},
}

@article{han_tree-based_2016,
	title = {Tree-{Based} {Visualization} and {Optimization} for {Image} {Collection}},
	volume = {46},
	issn = {21682267},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937232110&doi=10.1109%2fTCYB.2015.2448236&partnerID=40&md5=f1a9c12327774099bb9a6fc78923fb51},
	doi = {10.1109/TCYB.2015.2448236},
	abstract = {The visualization of an image collection is the process of displaying a collection of images on a screen under some specific layout requirements. This paper focuses on an important problem that is not well addressed by the previous methods: visualizing image collections into arbitrary layout shapes while arranging images according to user-defined semantic or visual correlations (e.g., color or object category). To this end, we first propose a property-based tree construction scheme to organize images of a collection into a tree structure according to user-defined properties. In this way, images can be adaptively placed with the desired semantic or visual correlations in the final visualization layout. Then, we design a two-step visualization optimization scheme to further optimize image layouts. As a result, multiple layout effects including layout shape and image overlap ratio can be effectively controlled to guarantee a satisfactory visualization. Finally, we also propose a tree-Transfer scheme such that visualization layouts can be adaptively changed when users select different 'images of interest.' We demonstrate the effectiveness of our proposed approach through the comparisons with state-of-The-Art visualization techniques. © 2013 IEEE.},
	language = {English},
	number = {6},
	journal = {IEEE Transactions on Cybernetics},
	author = {Han, Xintong and Zhang, Chongyang and Lin, Weiyao and Xu, Mingliang and Sheng, Bin and Mei, Tao},
	year = {2016},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Visualization, Semantics, Image collections, Trees (mathematics), State of the art, Object categories, Optimization scheme, Transfer scheme, Tree construction, Visual correlations, Visualization technique},
	pages = {1286 -- 1300},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\CND77V5S\\Han et al. - 2016 - Tree-Based Visualization and Optimization for Imag.pdf:application/pdf},
}

@article{crissaff_aries_2018,
	title = {{ARIES}: {Enabling} visual exploration and organization of art image collections},
	volume = {38},
	issn = {02721716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031794586&doi=10.1109%2fMCG.2017.377152546&partnerID=40&md5=82f8ffa339e4df6634aec750ab34d60e},
	doi = {10.1109/MCG.2017.377152546},
	abstract = {Art historians have traditionally used physical light boxes to prepare exhibits or curate collections. On a light box, they can place slides or printed images, move the images around at will, group them as desired, and visual-ly compare them. The transition to digital images has rendered this workflow obsolete. Now, art historians lack well-designed, unified interactive software tools that effectively support the operations they perform with physi-cal light boxes. To address this problem, we designed ARIES (ARt Image Exploration Space), an interactive image manipulation system that enables the exploration and organization of fine digital art. The system allows images to be compared in multiple ways, offering dynamic overlays analogous to a physical light box, and sup-porting advanced image comparisons and feature-matching functions, available through computational image processing. We demonstrate the effectiveness of our system to support art historians tasks through real use cases. © 1981-2012 IEEE.},
	language = {English},
	number = {1},
	journal = {IEEE Computer Graphics and Applications},
	author = {Crissaff, Lhaylla and Wood Ruby, Louisa and Deutch, Samantha and Dubois, R. Luke and Fekete, Jean-Daniel and Freire, Juliana and Silva, Claudio},
	year = {2018},
	pmid = {28991735},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {History, Visualization, Tools, Computer aided software engineering, User interfaces, Image processing, User experience, Image collections, Flow visualization, Art history, Digital image, Light sources, Societies and institutions, Space explorations, Space research, User interaction},
	pages = {91 -- 108},
	file = {Crissaff et al. - 2018 - ARIES Enabling visual exploration and organizatio.pdf:C\:\\Users\\Richard\\Zotero\\storage\\NHFAPT8K\\Crissaff et al. - 2018 - ARIES Enabling visual exploration and organizatio.pdf:application/pdf},
}

@article{gu_visualization_2017,
	title = {Visualization and recommendation of large image collections toward effective sensemaking},
	volume = {16},
	issn = {14738716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014294384&doi=10.1177%2f1473871616630778&partnerID=40&md5=d8ae6996487e9a3cf5d08447e174fd5a},
	doi = {10.1177/1473871616630778},
	abstract = {In our daily lives, images are among the most commonly found data which we need to handle. We present iGraph, a graph-based approach for visual analytics of large image collections and their associated text information. Given such a collection, we compute the similarity between images, the distance between texts, and the connection between image and text to construct iGraph, a compound graph representation which encodes the underlying relationships among these images and texts. To enable effective visual navigation and comprehension of iGraph with tens of thousands of nodes and hundreds of millions of edges, we present a progressive solution that offers collection overview, node comparison, and visual recommendation. Our solution not only allows users to explore the entire collection with representative images and keywords but also supports detailed comparison for understanding and intuitive guidance for navigation. The visual exploration of iGraph is further enhanced with the implementation of bubble sets to highlight group memberships of nodes, suggestion of abnormal keywords or time periods based on text outlier detection, and comparison of four different recommendation solutions. For performance speedup, multiple graphics processing units and central processing units are utilized for processing and visualization in parallel. We experiment with two image collections and leverage a cluster driving a display wall of nearly 50 million pixels. We show the effectiveness of our approach by demonstrating experimental results and conducting a user study. © The Author(s) 2015.},
	language = {English},
	number = {1},
	journal = {Information Visualization},
	author = {Gu, Yi and Wang, Chaoli and Ma, Jun and Nemiroff, Robert J. and Kao, David L. and Parra, Denis},
	year = {2017},
	note = {Publisher: SAGE Publications Ltd
Type: Article},
	keywords = {Visualization, Computer graphics, Graph layout, Image collections, Large images, Visual exploration, Graphics processing unit, Group memberships, node comparison, Program processors, Visual Navigation, visual recommendation},
	pages = {21 -- 47},
	file = {Gu et al. - 2017 - Visualization and recommendation of large image co.pdf:C\:\\Users\\Richard\\Zotero\\storage\\4ITX42CW\\Gu et al. - 2017 - Visualization and recommendation of large image co.pdf:application/pdf},
}

@inproceedings{mcparlane_picture_2014,
	title = {"{Picture} the scene⋯" visually summarising social media events},
	isbn = {978-1-4503-2598-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937597330&doi=10.1145%2f2661829.2661923&partnerID=40&md5=95a263f0d62e25350a721ad1720cae4f},
	doi = {10.1145/2661829.2661923},
	abstract = {Due to the advent of social media and web 2.0, we are faced with a deluge of information; recently, research efforts have focused on filtering out noisy, irrelevant information items from social media streams and in particular have attempted to automatically identify and summarise events. However, due to the heterogeneous nature of such social media streams, these efforts have not reached fruition. In this paper, we investigate how images can be used as a source for summarising events. Existing approaches have considered only textual summaries which are often poorly written, in a different language and slow to digest. Alternatively, images are "worth 1,000 words" and are able to quickly \& easily convey an idea or scene. Since images in social media can also be noisy, irrelevant \& repetitive, we propose new techniques for their automatic selection, ranking and presentation. We evaluate our approach on a recently created social media event data set containing 365k tweets and 50 events, for which we extend by collecting 625k related images. By conducting two crowdsourced evaluations, we firstly show how our approach overcomes the problems of automatically collecting relevant and diverse images from noisy microblog data, before highlighting the advantages of multimedia summarisation over text based approaches. Copyright 2014 ACM.},
	language = {English},
	booktitle = {{CIKM} 2014 - {Proceedings} of the 2014 {ACM} {International} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {Association for Computing Machinery},
	author = {McParlane, Philip J. and McMinn, Andrew J. and Jose, Joemon M.},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Social networking (online), Information filtering, Twitter, Knowledge management, Image diversification, Media streaming, Near-duplicate detection, Social media, Visual event summarisation},
	pages = {1459 -- 1468},
	file = {McParlane et al. - 2014 - Picture the scene...\; Visually Summarising Soci.pdf:C\:\\Users\\Richard\\Zotero\\storage\\227WRHJ6\\McParlane et al. - 2014 - Picture the scene...\; Visually Summarising Soci.pdf:application/pdf},
}

@article{wilkinson_visualizing_2015,
	title = {Visualizing document image collections using image-based word clouds},
	volume = {9474},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952662294&doi=10.1007%2f978-3-319-27857-5_27&partnerID=40&md5=9e730ec1b3120ba36fbaf4100b62ebd5},
	doi = {10.1007/978-3-319-27857-5_27},
	abstract = {In this paper, we introduce image-based word clouds as a novel tool for a quick and aesthetic overviews of common words in collections of digitized text manuscripts. While OCR can be used to enable summaries and search functionality to printed modern text, historical and handwritten documents remains a challenge. By segmenting and counting word images, without applying manual transcription or OCR, we have developed a method that can produce word or tag clouds from document collections. Our new tool is not limited to any specific kind of text. We make further contributions in ways of stop-word removal, class based feature weighting and visualization. An evaluation of the proposed tool includes comparisons with ground truth word clouds on handwritten marriage licenses from the 17th century and the George Washington database of handwritten letters, from the 18th century. Our experiments show that image-based word clouds capture the same information, albeit approximately, as the regular word clouds based on text data. © Springer International Publishing Switzerland 2015.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Wilkinson, Tomas and Brun, Anders},
	editor = {M, Elendt and R, Boyle and E, Ragan and B, Parvin and R, Feris and T, McGraw and I, Pavlidis and R, Kopper and G, Bebis and D, Koracin and Z, Ye and G, Weber},
	year = {2015},
	note = {ISBN: 978-331927856-8
Publisher: Springer Verlag
Type: Conference paper},
	keywords = {Data mining, Artificial intelligence, Computers, Document images, Document collection, 18th century, Class-based, Feature weighting, Ground truth, Handwritten document, Search functionality},
	pages = {297 -- 306},
	file = {Wilkinson and Brun - 2015 - Visualizing document image collections using image.pdf:C\:\\Users\\Richard\\Zotero\\storage\\Y9G97XDM\\Wilkinson and Brun - 2015 - Visualizing document image collections using image.pdf:application/pdf},
}

@article{ryu_summarized_2014,
	title = {A summarized photo visualization system with maximal clique finding algorithm},
	volume = {73},
	issn = {13807501},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916943762&doi=10.1007%2fs11042-012-1160-7&partnerID=40&md5=17d5321c320de294dacdf6232e066688},
	doi = {10.1007/s11042-012-1160-7},
	abstract = {The affordability of digital cameras, storage, processors and the advances made in these areas are encouraging people to continuously take hundreds of photos. However, managing the large number of photographs involves arduous tasks such as selecting good quality photos and classifying and labeling each photo. Generally, users put their photos into certain user-designated folders on their local PCs without considering any classified information. One of the main problems related to this management method is that users do not systematically create their photo folders because they are careless and apathetic. This practice results in confusion when the users want to find their photos. One method to overcome this problem is to construct a central photo management system that can manage many photos on the user’s local PC. This paper proposes an integrated photo management system coupled with a database on the web, which provides users with an automated photo clustering and visualization function that allows photo overlaps. The proposed system provides spatial clustering for Nearly Identical Photos, and it places photos with overlaps to improve space efficiency for the user. This system also provides users with a CUDA version of Depth of Field evaluation and blur estimation functions. In order to evaluate our system, we conducted two quantitative experiments relating to space efficiency and clustering correctness. First, we investigate the placed photo areas of ACDSee (grid layout) and our system to evaluate how much screen space is saved by nearly identical photos overlapping. Second, we also calculate the precision and recall of our system and Cooper’s with regard to user-classified photo sets. © Springer Science+Business Media, LLC 2012.},
	language = {English},
	number = {2},
	journal = {Multimedia Tools and Applications},
	author = {Ryu, Dong-Sung and Cho, Hwan-Gue},
	year = {2014},
	note = {Publisher: Kluwer Academic Publishers
Type: Article},
	keywords = {Visualization, Image enhancement, Classified information, Efficiency, Function evaluation, Maximal clique findings, Microcomputers, Nearly identical photo, Photo clustering, Photo management, Photo management systems, Quantitative experiments, Visualization functions},
	pages = {1011 -- 1027},
	file = {A-summarized-photo-visualization-system-with-maximal-clique-finding-algorithmMultimedia-Tools-and-Applications.pdf:C\:\\Users\\Richard\\Zotero\\storage\\GRUMUQ36\\A-summarized-photo-visualization-system-with-maximal-clique-finding-algorithmMultimedia-Tools-and-Applications.pdf:application/pdf},
}

@article{hoque_combining_2013,
	title = {Combining conceptual query expansion and visual search results exploration for web image retrieval},
	volume = {4},
	issn = {18685145},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878537451&doi=10.1007%2fs12652-011-0094-7&partnerID=40&md5=332d0effb8ded3a1ca41418ecc0bf262},
	doi = {10.1007/s12652-011-0094-7},
	abstract = {Most approaches to image retrieval on the web have their basis in document search techniques. Images are indexed based on the text that is related to the images. Queries are matched to this text to produce a set of search results, which are organized in paged grids that are reminiscent of lists of documents. Due to ambiguity both with user-supplied queries and with the text used to describe the images within the search index, most image searches contain many irrelevant images distributed throughout the search results, and are often focused on the most common interpretation of the query. We propose a method for addressing these problems in which conceptual query expansion is used to generate a diverse range of images, and a multi-resolution extension of a self-organizing map is used to group visually similar images. The resulting interface acts as an intelligent search assistant, automatically diversifying the search results and then allowing the searcher to interactively highlight and filter images based on the concepts, and zoom into an area within the image space to show additional images that are visually similar. Evaluations show that the precision of the image search results increase as a result of concept-based focusing and filtering, as well as visual zooming operations, even for uncommon interpretations of ambiguous queries. © 2011 Springer-Verlag.},
	language = {English},
	number = {3},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Hoque, Enamul and Hoeber, Orland and Strong, Grant and Gong, Minglun},
	year = {2013},
	note = {Type: Article},
	keywords = {Image retrieval, Conformal mapping, Interactive exploration, Conceptual queries, Document search, Image search, Intelligent search, Multi-resolutions, Web image retrieval, Zooming operation},
	pages = {389 -- 400},
	file = {Hoque et al. - 2013 - Combining conceptual query expansion and visual se.pdf:C\:\\Users\\Richard\\Zotero\\storage\\I2TGS8HG\\Hoque et al. - 2013 - Combining conceptual query expansion and visual se.pdf:application/pdf},
}

@article{zahalka_ii-20_2021,
	title = {{II}-20: {Intelligent} and pragmatic analytic categorization of image collections},
	volume = {27},
	issn = {10772626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100415947&doi=10.1109%2fTVCG.2020.3030383&partnerID=40&md5=e2026d9ecd84238fa9f307f5e3ca7a2f},
	doi = {10.1109/TVCG.2020.3030383},
	abstract = {In this paper, we introduce 11-20 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11-20 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand ('fast-forward') the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II-20 is an intuitive, efficient, and effective multimedia analytics tool. © 2020 IEEE.},
	language = {English},
	number = {2},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Zahalka, Jan and Worring, Marcel and Van Wijk, Jarke J.},
	year = {2021},
	pmid = {33074815},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {Analytical models, Semantics, Image collections, Learning systems, Advanced visualizations, Flexible machines, Interactive learning, Machine classifications, Minimal interactions, Semantic interactions, Tight integrations, Turing machines},
	pages = {422 -- 431},
	file = {Zahalka et al. - 2021 - II-20 Intelligent and pragmatic analytic categori.pdf:C\:\\Users\\Richard\\Zotero\\storage\\SRMKI2TB\\Zahalka et al. - 2021 - II-20 Intelligent and pragmatic analytic categori.pdf:application/pdf},
}

@article{fang_narrative_2018,
	title = {Narrative {Collage} of {Image} {Collections} by {Scene} {Graph} {Recombination}},
	volume = {24},
	issn = {10772626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031824417&doi=10.1109%2fTVCG.2017.2759265&partnerID=40&md5=4e8ece1da216d5b3ee772ce6ee76c62d},
	doi = {10.1109/TVCG.2017.2759265},
	abstract = {A narrative collage is an interesting image editing method for summarizing the main theme or storyline behind an image collection. We present a novel method to generate narrative images with plausible semantic scene structures. To achieve this goal, we introduce a layer graph and a scene graph to represent the relative depth order and semantic relationship between image objects, respectively. We first cluster the input image collection to select representative images, and then we extract a group of semantic salient objects from each representative image. Both layer graphs and scene graphs are constructed and combined according to our specific rules for reorganizing the extracted objects in every image. We design an energy model to appropriately locate every object on the final canvas. The experimental results show that our method can produce competitive narrative collage results and that it performs well on a wide range of image collections. © 1995-2012 IEEE.},
	language = {English},
	number = {9},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fang, Fei and Yi, Miao and Feng, Hui and Hu, Shenghong and Xiao, Chunxia},
	year = {2018},
	pmid = {28981417},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {Layout, Image segmentation, Semantics, Database systems, Image collections, Flow visualization, Image generations, Image synthesis, Narrative Collage, Painting, Scene graph},
	pages = {2559 -- 2572},
	file = {Fang et al. - 2018 - Narrative Collage of Image Collections by Scene Gr.pdf:C\:\\Users\\Richard\\Zotero\\storage\\B45EA4PG\\Fang et al. - 2018 - Narrative Collage of Image Collections by Scene Gr.pdf:application/pdf},
}

@article{wang_similarity-based_2015,
	title = {Similarity-based visualization of large image collections},
	volume = {14},
	issn = {14738716},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943253662&doi=10.1177%2f1473871613498519&partnerID=40&md5=0e1c5f3264c385041d825ac554db6d81},
	doi = {10.1177/1473871613498519},
	abstract = {Effective techniques for organizing and visualizing large image collections are in growing demand as visual search gets increasingly popular. Targeting an online astronomy archive with thousands of images, we present our solution for image search and clustering based on the evaluation of image similarity using both visual and textual information. Time-consuming image similarity computation is accelerated using graphics processing unit. To lay out images, we introduce iMap, a treemap-based representation for visualizing and navigating image search and clustering results. iMap not only makes effective use of available display area to arrange images but also maintains stable update when images are inserted or removed during the query. We also develop an embedded visualization that integrates image tags for in-place search refinement. To show the effectiveness of our approach, we demonstrate experimental results, compare our iMap layout with a force-directed layout, and conduct a comparative user study. As a potential tool for astronomy education and outreach, we deploy our iMap to a large tiled display of nearly 50 million pixels. © The Author(s) 2013.},
	language = {English},
	number = {3},
	journal = {Information Visualization},
	author = {Wang, Chaoli and Reese, John P. and Zhang, Huan and Tao, Jun and Gu, Yi and Ma, Jun and Nemiroff, Robert J.},
	year = {2015},
	note = {Publisher: SAGE Publications Ltd
Type: Article},
	keywords = {Visualization, Clustering algorithms, Image analysis, Computer graphics, Image collections, Graphics processing unit, Program processors, Astronomy educations, Embedded search, Force-directed layout, Image layout, Image similarity computation, Similarity-based visualizations, Tiled display},
	pages = {183 -- 203},
	file = {Wang et al. - 2015 - Similarity-based visualization of large image coll.pdf:C\:\\Users\\Richard\\Zotero\\storage\\UFDBF4ZK\\Wang et al. - 2015 - Similarity-based visualization of large image coll.pdf:application/pdf},
}

@article{pan_content-based_2021,
	title = {Content-{Based} {Visual} {Summarization} for {Image} {Collections}},
	volume = {27},
	issn = {10772626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102055610&doi=10.1109%2fTVCG.2019.2948611&partnerID=40&md5=51bc8ccb9b8863ed0be42c09a44b4574},
	doi = {10.1109/TVCG.2019.2948611},
	abstract = {With the surge of images in the information era, people demand an effective and accurate way to access meaningful visual information. Accordingly, effective and accurate communication of information has become indispensable. In this article, we propose a content-based approach that automatically generates a clear and informative visual summarization based on design principles and cognitive psychology to represent image collections. We first introduce a novel method to make representative and nonredundant summarizations of image collections, thereby ensuring data cleanliness and emphasizing important information. Then, we propose a tree-based algorithm with a two-step optimization strategy to generate the final layout that operates as follows: (1) an initial layout is created by constructing a tree randomly based on the grouping results of the input image set; (2) the layout is refined through a coarse adjustment in a greedy manner, followed by gradient back propagation drawing on the training procedure of neural networks. We demonstrate the usefulness and effectiveness of our method via extensive experimental results and user studies. Our visual summarization algorithm can precisely and efficiently capture the main content of image collections better than alternative methods or commercial tools. © 1995-2012 IEEE.},
	language = {English},
	number = {4},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Pan, Xingjia and Tang, Fan and Dong, Weiming and Ma, Chongyang and Meng, Yiping and Huang, Feiyue and Lee, Tong-Yee and Xu, Changsheng},
	year = {2021},
	pmid = {31647438},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {algorithm, psychology, Image collections, Visual information, Backpropagation, article, human experiment, human, artificial neural network, back propagation, Cognitive psychology, Content-based approach, Design Principles, drawing, Training procedures, Tree-based algorithms, Trees (mathematics), Two-step optimizations},
	pages = {2298 -- 2312},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Richard\\Zotero\\storage\\NWL9UC8J\\8880504.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\NYYFM29N\\Pan et al. - 2021 - Content-Based Visual Summarization for Image Colle.pdf:application/pdf},
}

@inproceedings{barthel_real-time_2019,
	title = {Real-time visual navigation in huge image sets using similarity graphs},
	isbn = {978-1-4503-6889-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074871490&doi=10.1145%2f3343031.3350599&partnerID=40&md5=3b5f194213bdd7c9dfaee6ce946256d5},
	doi = {10.1145/3343031.3350599},
	abstract = {Nowadays stock photo agencies often have millions of images. Nonstop viewing of 20 million images at a speed of 10 images per second would take more than three weeks. This demonstrates the impossibility to inspect all images and the difficulty to get an overview of the entire collection. Although there has been a lot of effort to improve visual image search, there is little research and support for visual image exploration. Typically, users start "exploring" an image collection with a keyword search or an example image for a similarity search. Both searches lead to long unstructured lists of result images. In earlier publications, we introduced the idea of graph-based image navigation and proposed an efficient algorithm for building hierarchical image similarity graphs for dynamically changing image collections. In this demo we showcase real-time visual exploration of millions of images with a standard web browser. Subsets of images are successively retrieved from the graph and displayed as a visually sorted 2D image map, which can be zoomed and dragged to explore related concepts. Maintaining the positions of previously shown images creates the impression of an "endless map". This approach allows an easy visual image-based navigation, while preserving the complex image relationships of the graph. © 2019 Association for Computing Machinery.},
	language = {English},
	booktitle = {{MM} 2019 - {Proceedings} of the 27th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Barthel, Kai Uwe and Hezel, Nico and Schall, Konstantin and Jung, Klaus},
	year = {2019},
	note = {Type: Conference paper},
	keywords = {Navigation, Visualization, Image retrieval, Similarity search, Image collections, Image enhancement, Flow visualization, Search engines, Visual exploration, Image similarity, Visual Navigation, Air navigation, Image graphs, Image navigation, Keyword search, Natural resources exploration},
	pages = {2202 -- 2204},
	file = {Barthel et al. - 2019 - Real-time visual navigation in huge image sets usi.pdf:C\:\\Users\\Richard\\Zotero\\storage\\CCUJX3GV\\Barthel et al. - 2019 - Real-time visual navigation in huge image sets usi.pdf:application/pdf},
}

@inproceedings{kim_generating_2015,
	title = {Generating summaries for photographic images based on human affects},
	isbn = {978-1-4673-7289-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960931377&doi=10.1109%2fICCI-CC.2015.7259411&partnerID=40&md5=f1341e351a2e5f86558abc2ebbe15b43},
	doi = {10.1109/ICCI-CC.2015.7259411},
	abstract = {The selection of canonical images that best represent a scene type is very important for efficiently visualizing search results and re-ranking them. The canonical images can be obtained using various aspects including viewpoint, visual features, and semantics. Here, we propose the selection of canonical images based on human affects. The proposed method is performed using three steps: extract the affective features from the input image, cluster images in the affective space and rank the clusters, and find representative images within each cluster. First, the probabilistic affective model is used to transform the images into the affective space. Thereafter, the images are clustered in the affective space. Then, the selected canonical images are representative and distinctive from each other. Thus, we define three prominent properties that an informative summary should satisfy: coverage, affective coherence, and distinctiveness. Based on these, cluster ranking is performed. Finally, the representative images for each cluster are selected, all of which are displayed as canonical images to the user. Experiments using web image databases demonstrate are not only representative but also exhibit a diverse set of views with minimal redundancy. © 2015 IEEE.},
	language = {English},
	booktitle = {Proceedings of 2015 {IEEE} 14th {International} {Conference} on {Cognitive} {Informatics} and {Cognitive} {Computing}, {ICCI}*{CC} 2015},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kim, Eun Yi and Ko, Eunjeong},
	editor = {P, Chen and L.A, Zadeh and N, Ge and Y, Wang and X, Tao and J, Lu and N, Howard and B, Zhang},
	year = {2015},
	note = {Type: Conference paper},
	keywords = {Semantics, Information science, Affective model, cluster ranking, human affects, Image selection, Input image, Minimal redundancy, Photographic image, Photography, Visual feature},
	pages = {360 -- 367},
	file = {Kim and Ko - 2015 - Generating summaries for photographic images based.pdf:C\:\\Users\\Richard\\Zotero\\storage\\E5WWGUQI\\Kim and Ko - 2015 - Generating summaries for photographic images based.pdf:application/pdf},
}

@article{sanderson_ceiba_2014,
	title = {Ceiba: {Scalable} visualization of phylogenies and {2D}/{3D} image collections},
	volume = {30},
	issn = {13674803},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907021027&doi=10.1093%2fbioinformatics%2fbtu315&partnerID=40&md5=872c373f419e0d07ece5f3bfdc3e7005},
	doi = {10.1093/bioinformatics/btu315},
	abstract = {Summary: Phylogenetic trees with hundreds of thousands of leaves are now being inferred from sequence data, posing significant challenges for visualization and exploratory analysis. Image data supplying valuable context for species in trees (and cues for exploring them) are becoming increasingly available in biodiversity databases and elsewhere but have rarely been built into tree visualization software in a scalable way. Ceiba lets the user explore large trees and inspect image collection arrays (sets of 'homologous' images) comprising mixtures of 2D and 3D image objects. Ceiba exploits recent improvements in graphics hardware, OpenGL toolkits and many standard high-performance computer graphics strategies, such as texture compression, level of detail control, culling, animations and image caching. Its tree layouts can be tuned by user-provided phylogenetic definitions of subtrees. The code has been extensively tested on phylogenies of up to 55 000 leaves and images. © The Author 2014. Published by Oxford University Press. All rights reserved.},
	language = {English},
	number = {17},
	journal = {Bioinformatics},
	author = {Sanderson, Michael J.},
	year = {2014},
	pmid = {24813216},
	note = {Publisher: Oxford University Press
Type: Article},
	keywords = {computer graphics, Computer Graphics, Imaging, Software, Phylogeny, User-Computer Interface, Computer-Assisted, phylogeny, image processing, Image Processing, computer interface, Ceiba, computer program, three dimensional imaging, Three-Dimensional},
	pages = {2506 -- 2507},
	file = {Sanderson - 2014 - Ceiba Scalable visualization of phylogenies and 2.pdf:C\:\\Users\\Richard\\Zotero\\storage\\4WCX6IPB\\Sanderson - 2014 - Ceiba Scalable visualization of phylogenies and 2.pdf:application/pdf},
}

@inproceedings{yu_joint_2014,
	title = {A joint optimization model for image summarization based on image content and tags},
	volume = {1},
	isbn = {978-1-57735-677-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908150528&partnerID=40&md5=e7c18afeafcf24ed406828a310aa7a13},
	abstract = {As an effective technology for navigating a large number of images, image summarization is becoming a promising task with the rapid development of image sharing sites and social networks. Most existing summarization approaches use the visual-based features for image representation without considering tag information. In this paper, we propose a novel framework, named JOINT, which employs both image content and tag information to summarize images. Our model generates the summary images which can best reconstruct the original collection. Based on the assumption that an image with representative content should also have typical tags, we introduce a similarity-inducing regularizer to our model. Furthermore, we impose the lasso penalty on the objective function to yield a concise summary set. Extensive experiments demonstrate our model outperforms the state-of-the-art approaches. Copyright © 2014, Association for the Advancement of Artificial Intelligence.},
	language = {English},
	booktitle = {Proceedings of the {National} {Conference} on {Artificial} {Intelligence}},
	publisher = {AI Access Foundation},
	author = {Yu, Hongliang and Deng, Zhi-Hong and Yang, Yunlun and Xiong, Tao},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Image summarization, Image content, Joint optimization},
	pages = {215 -- 221},
	file = {Yu et al. - 2014 - A joint optimization model for image summarization.pdf:C\:\\Users\\Richard\\Zotero\\storage\\L7T4IWL2\\Yu et al. - 2014 - A joint optimization model for image summarization.pdf:application/pdf},
}

@inproceedings{pogorelov_clustertag_2017,
	title = {{ClusterTag}: {Interactive} visualization, clustering and tagging tool for big image collections},
	isbn = {978-1-4503-4701-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021790565&doi=10.1145%2f3078971.3079018&partnerID=40&md5=a40e3cee0ea97511fcf7d884d855ec6a},
	doi = {10.1145/3078971.3079018},
	abstract = {Exploring and annotating collections of images without meta-data is a complex task which requires convenient ways of presenting datasets to a user. Visual analytics and information visualization can help users by providing interfaces, and in this paper, we present an open source application that allows users from any domain to use feature-based clustering of large image collections to perform explorative browsing and annotation. For this, we use various image feature extraction mechanisms, different unsupervised clustering algorithms and hierarchical image collection visualization. The performance of the presented open source software allows users to process and display thousands of images at the same time by utilizing heterogeneous resources such as GPUs and different optimization techniques. © 2017 Copyright held by the owner/author(s). ACM.},
	language = {English},
	booktitle = {{ICMR} 2017 - {Proceedings} of the 2017 {ACM} {International} {Conference} on {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Pogorelov, Konstantin and Riegler, Michael and Halvorsen, Pål and Griwodz, Carsten},
	year = {2017},
	note = {Type: Conference paper},
	keywords = {Visualization, Clustering algorithms, Information systems, Software engineering, Open source software, Image processing, Annotation, Clustering, Open systems, Flow visualization, Program processors, Big collections, Image browsing, Open sources},
	pages = {112 -- 116},
	file = {Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\T29382RL\\Pogorelov et al. - 2017 - ClusterTag Interactive Visualization, Clustering .pdf:application/pdf},
}

@inproceedings{seah_prism_2014,
	title = {{PRISM}: {Concept}-preserving social image search results summarization},
	isbn = {978-1-4503-2259-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904564432&doi=10.1145%2f2600428.2609586&partnerID=40&md5=a064e5f4eaf72ce96d318d80d6f890ed},
	doi = {10.1145/2600428.2609586},
	abstract = {Most existing tag-based social image search engines present search results as a ranked list of images, which cannot be consumed by users in a natural and intuitive manner. In this paper, we present a novel concept-preserving image search results summarization algorithm named prism. prism exploits both visual features and tags of the search results to generate high quality summary, which not only breaks the results into visually and semantically coherent clusters but it also maximizes the coverage of the summary w.r.t the original search results. It first constructs a visual similarity graph where the nodes are images in the search results and the edges represent visual similarities between pairs of images. This graph is optimally decomposed and compressed into a set of concept-preserving subgraphs based on a set of summarization objectives. Images in a concept-preserving subgraph are visually and semantically cohesive and are described by a minimal set of tags or concepts. Lastly, one or more exemplar images from each subgraph is selected to form the exemplar summary of the result set. Through empirical study, we demonstrate the effectiveness of prism against state-of-the-art image summarization and clustering algorithms. Copyright 2014 ACM.},
	language = {English},
	booktitle = {{SIGIR} 2014 - {Proceedings} of the 37th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Seah, Boon-Siew and Bhowmick, Sourav S. and Sun, Aixin},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Clustering algorithms, Information retrieval, Flickr, Empirical studies, Image summarization, Visual feature, Search engines, Social image searches, Image search, Coherent clusters, Prisms, Visual similarity},
	pages = {737 -- 746},
	file = {Seah et al. - 2014 - PRISM Concept-preserving social image search resu.pdf:C\:\\Users\\Richard\\Zotero\\storage\\L5T8DUM8\\Seah et al. - 2014 - PRISM Concept-preserving social image search resu.pdf:application/pdf},
}

@article{samani_knowledge-based_2017,
	title = {A knowledge-based semantic approach for image collection summarization},
	volume = {76},
	issn = {13807501},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988418733&doi=10.1007%2fs11042-016-3840-1&partnerID=40&md5=d3dec204865becdb715e6eeac11c00b2},
	doi = {10.1007/s11042-016-3840-1},
	abstract = {With the advent of digital cameras, the number of digital images is on the increase. As a result, image collection summarization systems are proposed to provide users with a condense set of summary images as a representative set to the original high volume image set. In this paper, a semantic knowledge-based approach for image collection summarization is presented. Despite ontology and knowledge-based systems have been applied in other areas of image retrieval and image annotation, most of the current image summarization systems make use of visual or numeric metrics for conducting the summarization. Also, some image summarization systems jointly model visual data of images together with their accompanying textual or social information, while these side data are not available out of the context of web or social images. The main motivation of using ontology approach in this study is its ability to improve the result of computer vision tasks by the additional knowledge which it provides to the system. We defined a set of ontology based features to measure the amount of semantic information contained in each image. A semantic similarity graph was made based on semantic similarities. Summary images were then selected based on graph centrality on the similarity graph. Experimental results showed that the proposed approach worked well and outperformed the current image summarization systems. © 2016, Springer Science+Business Media New York.},
	language = {English},
	number = {9},
	journal = {Multimedia Tools and Applications},
	author = {Samani, Zahra Riahi and Moghaddam, Mohsen Ebrahimi},
	year = {2017},
	note = {Publisher: Springer New York LLC
Type: Article},
	keywords = {Semantics, Computer vision, Image retrieval, Knowledge based systems, Image processing, Ontology, Image collections, Image summarization, Natural language processing systems, Search engines, Additional knowledge, Graph centralities, Semantic information, Semantic knowledge, Semantic similarity, Summarization systems},
	pages = {11917 -- 11939},
	file = {Samani and Moghaddam - 2017 - A knowledge-based semantic approach for image coll.pdf:C\:\\Users\\Richard\\Zotero\\storage\\62EW9VU4\\Samani and Moghaddam - 2017 - A knowledge-based semantic approach for image coll.pdf:application/pdf},
}

@article{phueaksri_approach_2023,
	title = {An {Approach} to {Generate} a {Caption} for an {Image} {Collection} {Using} {Scene} {Graph} {Generation}},
	volume = {11},
	issn = {21693536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177065586&doi=10.1109%2fACCESS.2023.3332098&partnerID=40&md5=5a49f0bd442bd9250d77ad742ad04f43},
	doi = {10.1109/ACCESS.2023.3332098},
	abstract = {Summarization is a challenging task that aims to generate a summary by grasping common information of a given set of information. Text summarization is a popular task of determining the topic or generating a textual summary of documents. In contrast, image summarization aims to find a representative summary of a collection of images. However, current methods are still restricted to generating a visual scene graph, tags, and noun phrases, but cannot generate a fitting textual description of an image collection. Thus, we introduce a novel framework for generating a summarized caption of an image collection. Since scene graph generation shows advancement in describing objects and their relationships on a single image, we use it in the proposed method to generate a scene graph for each image in an image collection. Then, we find common objects and their relationships from all scene graphs and represent them as a summarized scene graph. For this, we merge all scene graphs and select part of it by estimating the most common objects and relationships. Finally, the summarized scene graph is input into a captioning model. In addition, we introduce a technique to generalize specific words in the final caption into common concept words incorporating external knowledge. To evaluate the proposed method, we construct a dataset for this task by extending the annotation of the MS-COCO dataset using an image retrieval method. The evaluation of the proposed method on this dataset showed promising performance compared to text summarization-based methods. © ; 2023 The Authors.},
	language = {English},
	journal = {IEEE Access},
	author = {Phueaksri, Itthisak and Kastner, Marc A. and Kawanishi, Yasutomo and Komamizu, Takahiro and Ide, Ichiro},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Image edge detection, Task analysis, Semantics, Image analysis, Image retrieval, Text processing, Edge detection, Image captures, Image collection captioning, Image collections, Image summarization, Image-analysis, Job analysis, Multiple image, Multiple image summarization, Scene-graph summarization, Scene-graphs, Semantic summarization},
	pages = {128245 -- 128260},
	file = {Phueaksri et al. - 2023 - An Approach to Generate a Caption for an Image Col.pdf:C\:\\Users\\Richard\\Zotero\\storage\\BUSWG68N\\Phueaksri et al. - 2023 - An Approach to Generate a Caption for an Image Col.pdf:application/pdf},
}

@inproceedings{chavarro_visualizing_2013,
	title = {Visualizing multimodal image collections},
	isbn = {978-1-4799-1121-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891102120&doi=10.1109%2fSTSIVA.2013.6644945&partnerID=40&md5=bcc2753dabede9684990e6961564fdbe},
	doi = {10.1109/STSIVA.2013.6644945},
	abstract = {This paper presents two different strategies for visualizing multimodal image collections, which are based on a representation strategy that fuses text and visual content in the same latent space. This latent space allows to find semantic groups of images, which are used to select image prototypes to build a semantic visualization. The first strategy is a graph-based visualization in which edges represent image similarities and vertices represent images. The second is a multimodal visualization in which a set of image prototypes surround a semantic tag cloud. Thus, we built a system prototype in order to evaluate the strategies. Results show that the propose strategy is promising and it could be used in a real image exploration system to improve the image collection exploration process. © 2013 IEEE.},
	language = {English},
	booktitle = {Symposium of {Signals}, {Images} and {Artificial} {Vision} - 2013, {STSIVA} 2013},
	author = {Chavarro, Anyela and Camargo, Jorge and Gonzalez, Fabio A.},
	year = {2013},
	note = {Type: Conference paper},
	keywords = {Visualization, Semantics, Vision, Signal processing, Image collections, Multi-modal image, Latent factor analysis, Exploration systems, Graph-based visualization, Multi-modal visualization, Semantic visualization, summarization},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Richard\\Zotero\\storage\\VAZQ7NZT\\6644945.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\UBMAY7CU\\Chavarro et al. - 2013 - Visualizing multimodal image collections.pdf:application/pdf},
}

@inproceedings{van_der_corput_iclic_2016,
	title = {{ICLIC}: {Interactive} categorization of large image collections},
	volume = {2016-May},
	isbn = {978-1-5090-1451-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973641545&doi=10.1109%2fPACIFICVIS.2016.7465263&partnerID=40&md5=f993d2ca72381a1b5f320553d3fe0a62},
	doi = {10.1109/PACIFICVIS.2016.7465263},
	abstract = {We present a new approach for the analysis of large image collections. We argue that categorization plays an important role in this process, not only to label images as end result, but also during exploration. Furthermore, to increase the effectiveness and efficiency of the categorization process we enable the use of all available metadata, treated as multivariate data. We identified images, attributes, and categories as important aspects and integrated these in a system called ICLIC. The system consists of four views that are connected by the selection of images, which is a central action in the approach. By using a minimalist interface with only standard metaphors, users are enabled to use the system in short time. The system enables complex queries in a natural way, and it can deal with collections containing more than 100,000 images and more than 1,000 metadata attributes. This was confirmed by two evaluation cycles with domain experts. © 2016 IEEE.},
	language = {English},
	booktitle = {{IEEE} {Pacific} {Visualization} {Symposium}},
	publisher = {IEEE Computer Society},
	author = {Van Der Corput, Paul and Van Wijk, Jarke J.},
	editor = {C, Hansen and I, Viola and X, Yuan},
	year = {2016},
	note = {ISSN: 21658765
Type: Conference paper},
	keywords = {Visualization, Information systems, Graphical user interfaces, User interfaces, Metadata, Multivariate data, Interaction styles, Complex queries, Domain experts, Effectiveness and efficiencies, H.5.2 [information systems]: user interfaces, Label images, New approaches},
	pages = {152 -- 159},
	file = {Van Der Corput and Van Wijk - 2016 - ICLIC Interactive categorization of large image c.pdf:C\:\\Users\\Richard\\Zotero\\storage\\F7ICAC9J\\Van Der Corput and Van Wijk - 2016 - ICLIC Interactive categorization of large image c.pdf:application/pdf},
}

@inproceedings{nair_data_2018,
	title = {Data {Analysis} on {Multivariate} {Image} {Set}},
	isbn = {978-1-5386-7933-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059482762&doi=10.1109%2fNYSDS.2018.8538941&partnerID=40&md5=a6498606d31658a702b6dad6ee9879e5},
	doi = {10.1109/NYSDS.2018.8538941},
	abstract = {An image set can include not just the images themselves but also the extracted features, metadata and so on. For example, x-ray images obtained from synchrotron beamlines are large-scale highdynamic-range data depicting a variety of material properties after incorporating scientific analysis results. Previously, we presented a framework MultiSciView as an image set visualization and exploration system for x-ray scattering data. This tool is general enough to deal with any multivariate images. In this work, we aim to complement it with a set of data analysis modules. First, we present feature analysis by proposing a new correlation metric to reduce the data redundancy. Then we encode each image as a high dimensional vector and analyze the patterns hidden in the image set. Finally, we add an auxiliary visualization to plot the average and entropy images of the interested subset. We conducted one case study to show that our system can effectively analyze the image set, identify preferred image patterns, anomalous images and erroneous experimental settings. Eventually a better comprehension of the material nanostructure properties can be achieved. © 2018 IEEE.},
	language = {English},
	booktitle = {2018 {New} {York} {Scientific} {Data} {Summit}, {NYSDS} 2018 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nair, Shruti and Ha, Sungsoo and Xu, Wei},
	year = {2018},
	note = {Type: Conference paper},
	keywords = {Data visualization, Information analysis, Visualization, Image analysis, Data handling, Image datasets, Exploration systems, Data reduction, Experimental settings, Feature analysis, High dynamic range, Material nanostructure, Multivariant analysis, Scientific analysis, Synchrotron beamlines, X ray scattering},
	file = {Nair et al. - 2018 - Data Analysis on Multivariate Image Set.pdf:C\:\\Users\\Richard\\Zotero\\storage\\XZUMFIHW\\Nair et al. - 2018 - Data Analysis on Multivariate Image Set.pdf:application/pdf},
}

@article{chen_large_2016,
	title = {Large image collection visualization using perception-based similarity with color features},
	volume = {10072 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007042334&doi=10.1007%2f978-3-319-50835-1_35&partnerID=40&md5=63e4b4138a88e75fb703eacc9a695189},
	doi = {10.1007/978-3-319-50835-1_35},
	abstract = {This paper introduces the basic steps to build a similaritybased visualization tool for large image collections. We build the similarity metrics based on human perception. Psychophysical experiments have shown that human observers can recognize the gist of scenes within 100 milliseconds (ms) by comprehending the global properties of an image. Color also plays an important role in human rapid scene recognition. However, previous works often neglect color features. We propose new scene descriptors that preserve the information from coherent color regions, as well as the spatial layouts of scenes. Experiments show that our descriptors outperform existing state-of-the-art approaches. Given the similarity metrics, a hierarchical structure of an image collection can be built in a top-down manner. Representative images are chosen for image clusters and visualized using a force-directed graph. © Springer International Publishing AG 2016.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Chen, Zeyuan and Healey, Christopher G.},
	editor = {G, Bebis and B, Parvin and S, Skaff and D, Iwai and R, Boyle and D, Koracin and F, Porikli and C, Scheidegger and A, Entezari and J, Min and A, Sadagic and T, Isenberg},
	year = {2016},
	note = {ISBN: 978-331950834-4
Publisher: Springer Verlag
Type: Conference paper},
	keywords = {Visualization, Color, Directed graphs, Image collections, Similarity-based visualizations, Global properties, Hierarchical structures, Psychophysical experiments, Scene recognition, Similarity metrics, State-of-the-art approach},
	pages = {379 -- 390},
	file = {Chen and Healey - 2016 - Large image collection visualization using percept.pdf:C\:\\Users\\Richard\\Zotero\\storage\\QZC6D2YW\\Chen and Healey - 2016 - Large image collection visualization using percept.pdf:application/pdf},
}

@inproceedings{seah_prism_2015,
	title = {{PRISM}: {Concept}-preserving summarization of top-k social image search results},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953871445&doi=10.14778%2f2824032.2824088&partnerID=40&md5=729f8679326ba132cc42e9d3658954ae},
	doi = {10.14778/2824032.2824088},
	abstract = {Most existing tag-based social image search engines present search results as a ranked list of images, which cannot be consumed by users in a natural and intuitive manner. In this demonstration, we present a novel concept-preserving image search results summarization system called prism. prism exploits both visual features and tags of the search results to generate high quality summary, which not only breaks the results into visually and semantically coherent clusters but it also maximizes the coverage of the original top-k search results. It first constructs a visual similarity graph where the nodes are images in the top-k search results and the edges represent visual similarities between pairs of images. This graph is optimally decomposed and compressed into a set of concept-preserving subgraphs based on a set of summarization criteria. One or more exemplar images from each subgraph is selected to form the exemplar summary of the result set. We demonstrate various innovative features of prism and the promise of superior quality summary construction of social image search results. © 2015 VLDB Endowment 2150-8097/15/08.},
	language = {English},
	booktitle = {Proceedings of the {VLDB} {Endowment}},
	publisher = {Association for Computing Machinery},
	author = {Seah, Boon Siew and Bhowmick, Sourav S. and Sun, Aixin},
	year = {2015},
	note = {ISSN: 21508097
Issue: 12
Type: Book chapter},
	keywords = {Visual feature, Search engines, Social image searches, Image search, Coherent clusters, Prisms, Visual similarity, Summarization systems, High quality, Novel concept},
	pages = {1868 -- 1871},
	file = {Seah et al. - 2015 - PRISM Concept-preserving summarization of top-k s.pdf:C\:\\Users\\Richard\\Zotero\\storage\\R5I6WQCE\\Seah et al. - 2015 - PRISM Concept-preserving summarization of top-k s.pdf:application/pdf},
}

@inproceedings{zhang_method_2021,
	title = {Method for exploring generative adversarial networks (gans) via automatically generated image galleries},
	isbn = {978-1-4503-8096-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106731016&doi=10.1145%2f3411764.3445714&partnerID=40&md5=fee9c1a38808d081ac6727b3e9538b83},
	doi = {10.1145/3411764.3445714},
	abstract = {Generative Adversarial Networks (GANs) can automatically generate quality images from learned model parameters. However, it remains challenging to explore and objectively assess the quality of all possible images generated using a GAN. Currently, model creators evaluate their GANs via tedious visual examination of generated images sampled from narrow prior probability distributions on model parameters. Here, we introduce an interactive method to explore and sample quality images from GANs. Our frst two user studies showed that participants can use the tool to explore a GAN and select quality images. Our third user study showed that images sampled from a posterior probability distribution using a Markov Chain Monte Carlo (MCMC) method on parameters of images collected in our frst study resulted in on average higher quality and more diverse images than existing baselines. Our work enables principled qualitative GAN exploration and evaluation. © 2021 ACM.},
	language = {English},
	booktitle = {Conference on {Human} {Factors} in {Computing} {Systems} - {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Enhao and Banovic, Nikola},
	year = {2021},
	note = {Type: Conference paper},
	keywords = {Monte Carlo methods, Automatically generated, Human engineering, Adversarial networks, Interactive methods, Markov chain Monte Carlo method, Markov chains, Model parameters, Probability distributions, Quality image, Sample quality, Visual examination},
	file = {Zhang and Banovic - 2021 - Method for exploring generative adversarial networ.pdf:C\:\\Users\\Richard\\Zotero\\storage\\7MWMFFE9\\Zhang and Banovic - 2021 - Method for exploring generative adversarial networ.pdf:application/pdf},
}

@inproceedings{rematas_dataset_2015,
	title = {Dataset fingerprints: {Exploring} image collections through data mining},
	volume = {07-12-June-2015},
	isbn = {978-1-4673-6964-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959250728&doi=10.1109%2fCVPR.2015.7299120&partnerID=40&md5=6c27302408bf261585a6fb53ee3c7652},
	doi = {10.1109/CVPR.2015.7299120},
	abstract = {As the amount of visual data increases, so does the need for summarization tools that can be used to explore large image collections and to quickly get familiar with their content. In this paper, we propose dataset fingerprints, a new and powerful method based on data mining that extracts meaningful patterns from a set of images. The discovered patterns are compositions of discriminative midlevel features that co-occur in several images. Compared to earlier work, ours stands out because i) it's fully unsupervised, ii) discovered patterns cover large parts of the images, often corresponding to full objects or meaningful parts thereof, and iii) different patterns are connected based on co-occurrence, allowing a user to 'browse' the images from one pattern to the next and to group patterns in a semantically meaningful manner. © 2015 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Rematas, Konstantinos and Fernando, Basura and Dellaert, Frank and Tuytelaars, Tinne},
	year = {2015},
	note = {ISSN: 10636919
Type: Conference paper},
	keywords = {Data mining, Computer vision, Image collections, Co-occurrence, Large images, Mid-level features, Pattern recognition, Visual data},
	pages = {4867 -- 4875},
	file = {Rematas et al. - 2015 - Dataset fingerprints Exploring image collections .pdf:C\:\\Users\\Richard\\Zotero\\storage\\S44PZQKI\\Rematas et al. - 2015 - Dataset fingerprints Exploring image collections .pdf:application/pdf},
}

@article{zhao_visual_2016,
	title = {Visual summarization of image collections by fast {RANSAC}},
	volume = {172},
	issn = {09252312},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946497159&doi=10.1016%2fj.neucom.2014.09.095&partnerID=40&md5=af11fbd50cd66dfb0b210386eb25260a},
	doi = {10.1016/j.neucom.2014.09.095},
	abstract = {In this paper we propose a novel approach to select a summary set of images from a large image collection by improved Random Sample Consensus (RANSAC) and Affinity Propagation (AP) clustering. It can automatically select a small set of representatives to highlight all the significant visual properties of a given image collection. The proposed framework mainly composes four stages. First, the scale-invariant feature of each image is extracted by Scale Invariant Feature Transform (SIFT). Second, keypoints of two images are matched and ranked based on nearest neighbor ratio. The representative dataset of RANSAC is established by a minimal number of optimal matches. Third, the target homographic matrix is fitted based on the representative dataset. Mismatches are filtered out via the homographic matrix. Finally, summarization is automatically formulated as an optimization framework by AP clustering. We conduct experiments on a set of Paris which is consisting of 1000 images downloaded from Flickr. The results show that the proposed approach significantly outperforms other methods. © 2015 Elsevier B.V.},
	language = {English},
	journal = {Neurocomputing},
	author = {Zhao, Ye and Hong, Richang and Jiang, Jianguo},
	year = {2016},
	note = {Publisher: Elsevier
Type: Article},
	keywords = {Neural networks, cluster analysis, image processing, Iterative methods, affinity propagation, Affinity propagation, Article, controlled study, image analysis, intermethod comparison, mathematical analysis, mathematical computing, Nearest neighbor distance, Nearest neighbors, Optimization framework, priority journal, random sample consensus, Random sample consensus, Scale invariant feature transforms, Scale invariant features, Visual summarization},
	pages = {48 -- 52},
	file = {Zhao et al. - 2016 - Visual summarization of image collections by fast .pdf:C\:\\Users\\Richard\\Zotero\\storage\\3CSTI4J6\\Zhao et al. - 2016 - Visual summarization of image collections by fast .pdf:application/pdf},
}

@article{li_hybrid_2013,
	title = {Hybrid image summarization by hypergraph partition},
	volume = {119},
	issn = {18728286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881541600&doi=10.1016%2fj.neucom.2012.02.050&partnerID=40&md5=8f13250d11001928da6c9dc02c50e959},
	doi = {10.1016/j.neucom.2012.02.050},
	abstract = {The objective of hybrid image summarization is selecting a few visual exemplars and semantic exemplars of a large-scale image collection and organizing them to represent the collection. In this paper, we present a framework for hybrid image summarization in which social images and corresponding textual information are taken as vertices in a hypergraph and the task of image summarization is formulated as the problem of hypergraph partition. A generalized spectral clustering technique is adopted to solve the hypergraph partition problem. Besides, we design two representativeness score functions to select the visual exemplars and semantic exemplars. The main advantages of the proposed approach are two-fold: (1) the hypergraph framework takes advantage of homogeneous correlations within images and tags, respectively, as well as heterogeneous relations between them, this characteristic enhances the summarization performance; and (2) we take both visual and semantic representativeness into count to select exemplars, so that the image-tag exemplars are more representative for each cluster. The experimental comparisons to the other method are conducted on some common queries for a real internet image collection. User-based evaluation demonstrates the effectiveness of the proposed approach. © 2013 Elsevier B.V.},
	language = {English},
	journal = {Neurocomputing},
	author = {Li, Minxian and Zhao, Chunxia and Tang, Jinhui},
	year = {2013},
	note = {Type: Article},
	keywords = {Semantics, Neural networks, Internet, problem solving, cluster analysis, Hypergraph, vision, conceptual framework, visual information, image processing, Computer applications, Image summarization, article, human experiment, controlled study, intermethod comparison, priority journal, human, adult, computer interface, correlation analysis, Experimental comparison, female, Hybrid image, hybrid image summarization, Hypergraph partition, male, normal human, scoring system, semantics, Spectral clustering, Textual information, User-based evaluations},
	pages = {41 -- 48},
	file = {Li et al. - 2013 - Hybrid image summarization by hypergraph partition.pdf:C\:\\Users\\Richard\\Zotero\\storage\\QF9WYZUX\\Li et al. - 2013 - Hybrid image summarization by hypergraph partition.pdf:application/pdf},
}

@article{camargo_multimodal_2016,
	title = {Multimodal latent topic analysis for image collection summarization},
	volume = {328},
	issn = {00200255},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945569039&doi=10.1016%2fj.ins.2015.08.044&partnerID=40&md5=297d74dec45154bf645bc34f0cf747f0},
	doi = {10.1016/j.ins.2015.08.044},
	abstract = {This paper presents a multimodal latent topic analysis method for the construction of image collection summaries. The method automatically selects a set of prototypical images from a large set of retrieved images for a given query. We define an image collection summary as a subset of images from a collection, which is visually and semantically representative. To build such a summary we propose MICS (Multimodal Image Collection Summarization), a method that combines textual and visual modalities in a common latent space, which allows to find a subset of images from which the whole collection can be reconstructed. Experiments were conducted on two collections of tagged images demonstrating the ability of the approach to build summaries with representative visual and semantic contents. The method was evaluated using objective measures, reconstruction error and diversity of the summary, showing competitive results when compared to other summarization approaches. © 2015 Elsevier Inc. All rights reserved.},
	language = {English},
	journal = {Information Sciences},
	author = {Camargo, Jorge E. and González, Fabio A.},
	year = {2016},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Semantics, Image analysis, Content based retrieval, Image collections, Multi-modal image, Modal analysis, Multimodal analysis, NMF, Objective measure, Reconstruction error, Topic analysis, Visual modalities},
	pages = {270 -- 287},
	file = {Camargo and González - 2016 - Multimodal latent topic analysis for image collect.pdf:C\:\\Users\\Richard\\Zotero\\storage\\X76PI3NL\\Camargo and González - 2016 - Multimodal latent topic analysis for image collect.pdf:application/pdf},
}

@article{rodighiero_advanced_2023,
	title = {Advanced {Interface} {Design} for {IIIF} {A} {Digital} {Tool} to {Explore} {Image} {Collections} at {Different} {Scales}; [{Design} di interfacce avanzato per {IIIF}. {Uno} strumento digitale per esplorare collezioni di immagini a diverse scale]},
	volume = {2023},
	issn = {25328816},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179736991&doi=10.6092%2fissn.2532-8816%2f17230&partnerID=40&md5=d6ed67b2916eab68e2a2c787a8584931},
	doi = {10.6092/issn.2532-8816/17230},
	abstract = {This article introduces a proposal for an experimental interface design that uses the International Image Interoperability Framework (IIIF) to facilitate the exploration of image collections, relying on the relationships created by the scholarly practice of annotation. Within the project “From Data to Wisdom,” an innovative digital tool was designed by harnessing IIIF resources and leveraging close and distant reading on three levels of connectivity: micro, meso, and macro. The proposed tool integrates annotation features that enable scholars to analyze individual images as well as interpret broader connections and patterns across image sets. This article outlines the interface’s theoretical framework and design principles, highlighting the potential to support interdisciplinary research and advance digital art tools. © 2023, University of Bologna Department of Classical and Italian Philology, Alma Mater Studiorum. All rights reserved.},
	language = {English},
	number = {16},
	journal = {Umanistica Digitale},
	author = {Rodighiero, Dario and Romele, Alberto and Rubio, José Higuera and Pedro, Celeste and Azzi, Matteo and Uboldi, Giorgio},
	year = {2023},
	note = {Publisher: University of Bologna Department of Classical and Italian Philology, Alma Mater Studiorum
Type: Article},
	pages = {167 -- 192},
	file = {Rodighiero et al. - 2023 - Advanced Interface Design for IIIF A Digital Tool .pdf:C\:\\Users\\Richard\\Zotero\\storage\\Z8E8YDNT\\Rodighiero et al. - 2023 - Advanced Interface Design for IIIF A Digital Tool .pdf:application/pdf},
}

@article{pasini_semantic_2022,
	title = {Semantic {Image} {Collection} {Summarization} with {Frequent} {Subgraph} {Mining}},
	volume = {10},
	issn = {21693536},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146305307&doi=10.1109%2fACCESS.2022.3229654&partnerID=40&md5=72de34e5ddaea80e2f774ead8b4272a5},
	doi = {10.1109/ACCESS.2022.3229654},
	abstract = {Applications such as providing a preview of personal albums (e.g., Google Photos) or suggesting thematic collections based on user interests (e.g., Pinterest) require a semantically-enriched image representation, which should be more informative with respect to simple low-level visual features and image tags. To this aim, we propose an image collection summarization technique based on frequent subgraph mining. We represent images with a novel type of scene graphs including fine-grained relationship types between objects. These scene graphs are automatically derived by our method. The resulting summary consists of a set of frequent subgraphs describing the underlying patterns of the image dataset. Our results are interpretable and provide more powerful semantic information with respect to previous techniques, in which the summary is a subset of the collection in terms of images or image patches. The experimental evaluation shows that the proposed technique yields non-redundant summaries, with a high diversity of the discovered patterns. © 2013 IEEE.},
	language = {English},
	journal = {IEEE Access},
	author = {Pasini, Andrea and Giobergia, Flavio and Pastor, Eliana and Baralis, Elena},
	year = {2022},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Image representation, Data mining, Image segmentation, Semantics, Image collections, Scene-graphs, Frequent subgraph mining, Google+, Image collection summarization, Image representations, Panoptic segmentation, Semantic images, Simple++, Users' interests},
	pages = {131747 -- 131764},
	file = {Pasini et al. - 2022 - Semantic Image Collection Summarization with Frequ.pdf:C\:\\Users\\Richard\\Zotero\\storage\\9WGDZZYZ\\Pasini et al. - 2022 - Semantic Image Collection Summarization with Frequ.pdf:application/pdf},
}

@inproceedings{kim_discovering_2015,
	title = {Discovering collective narratives of theme parks from large collections of visitors' {Photo} streams},
	volume = {2015-August},
	isbn = {978-1-4503-3664-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954125335&doi=10.1145%2f2783258.2788569&partnerID=40&md5=96cc23b552763fe530f5dd8902b3b7cd},
	doi = {10.1145/2783258.2788569},
	abstract = {We present an approach for generating pictorial storylines from large collections of online photo streams shared by visitors to theme parks (e.g. Disneyland), along with publicly available information such as visitor's maps. The story graph visualizes various events and activities recurring across visitors' photo sets, in the form of hierarchically branching narrative structure associated with attractions and districts in theme parks. We first estimate story elements of each photo stream, including the detection of faces and supporting objects, and attraction-based localization. We then create spatio-temporal story graphs via an inference of sparse time-varying directed graphs. Through quantitative evaluation and crowdsourcing-based user studies via Amazon Mechanical Turk, we show that the story graphs serve as a more convenient mid-level data structure to perform photo-based recommendation tasks than other alternatives. We also present storybook-like demo examples regarding exploration, recommendation, and temporal analysis, which may be most beneficial uses of the story graphs to visitors. © 2015 ACM.},
	language = {English},
	booktitle = {Proceedings of the {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Kim, Gunhee and Sigal, Leonid},
	year = {2015},
	note = {Type: Conference paper},
	keywords = {Data mining, Quantitative evaluation, Graphic methods, Directed graphs, Amazon mechanical turks, Exhibitions, Multimedia data, Narrative structures, Spatio temporal, Storylines, Temporal analysis, User Modeling},
	pages = {1899 -- 1908},
	file = {Kim and Sigal - 2015 - Discovering collective narratives of theme parks f.pdf:C\:\\Users\\Richard\\Zotero\\storage\\9W42CDBN\\Kim and Sigal - 2015 - Discovering collective narratives of theme parks f.pdf:application/pdf},
}

@inproceedings{guldogan_personalized_2013,
	title = {Personalized representative image selection for shared photo albums},
	isbn = {978-1-4673-5285-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879849073&doi=10.1109%2fICCAT.2013.6522035&partnerID=40&md5=afd4d3580781eea3c9e5b072517e3e42},
	doi = {10.1109/ICCAT.2013.6522035},
	abstract = {In this paper, we present a novel method for personalizing the representative image selection using visual modeling and user statistics. For a given shared image album, a set of images that have the highest interest for a single user are determined. A personalized visual model is created using the given 'interest set' to be able to select the most representative image(s) for that user from that album. The experiments show satisfactory performance in the task of representative image selection. © 2013 IEEE.},
	language = {English},
	booktitle = {International {Conference} on {Computer} {Applications} {Technology}, {ICCAT} 2013},
	author = {Guldogan, Esin and Kangas, Jari and Gabbouj, Moncef},
	year = {2013},
	note = {Type: Conference paper},
	keywords = {Computer science, Computer applications, Image selection, Personalized summarization, Photo album, Single users, Visual model, Visual modeling},
	file = {Guldogan et al. - 2013 - Personalized representative image selection for sh.pdf:C\:\\Users\\Richard\\Zotero\\storage\\6CS2XHT8\\Guldogan et al. - 2013 - Personalized representative image selection for sh.pdf:application/pdf},
}

@inproceedings{low_visual_2014,
	title = {Visual berrypicking in large image collections},
	isbn = {1-59593-036-1 978-1-4503-2542-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911372348&doi=10.1145%2f2639189.2670271&partnerID=40&md5=ef0b346a439482012206a58713eaf8ba},
	doi = {10.1145/2639189.2670271},
	abstract = {Exploring image collections using similarity-based two-dimensional maps is an ongoing research area that faces two main challenges: with increasing size of the collection and complexity of the similarity metric projection accuracy rapidly degrades and computational costs prevent online map generation. We propose a prototype that creates the impression of panning a large (global) map by aligning inexpensive small maps showing local neighborhoods. By directed hopping from one neighborhood to the next the user is able to explore the whole image collection. Additionally, the similarity metric can be adapted by weighting image features and thus users benefit from a more informed navigation. Copyright is held by the owner/author(s).},
	language = {English},
	booktitle = {Proceedings of the {NordiCHI} 2014: {The} 8th {Nordic} {Conference} on {Human}-{Computer} {Interaction}: {Fun}, {Fast}, {Foundational}},
	publisher = {Association for Computing Machinery},
	author = {Low, Thomas and Hentschel, Christian and Stober, Sebastian and Sack, Harald and Nürnberger, Andreas},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Clustering algorithms, Image retrieval, Human computer interaction, Image collections, Interactive exploration, Similarity metrics, Computational costs, Local neighborhoods, Multi-dimensional scaling, Procrustes analysis, Two-dimensional map},
	pages = {1043 -- 1046},
	file = {Low et al. - 2014 - Visual berrypicking in large image collections.pdf:C\:\\Users\\Richard\\Zotero\\storage\\AUBPPX5R\\Low et al. - 2014 - Visual berrypicking in large image collections.pdf:application/pdf},
}

@inproceedings{rayar_incremental_2016,
	title = {Incremental hierarchical indexing and visualisation of large image collections},
	isbn = {978-2-87587-027-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994176885&partnerID=40&md5=76c74370ccdf11c4d9071401cab36c45},
	abstract = {Ever-growing image collections are common in several fields such as health, digital humanities or social networks. Nowadays, there is a lack of visualisation tools to browse such large image collection. In this work, the incremental indexing and the visualisation of large image collections is done jointly. The BIRCH algorithm is improved to incrementally yield a hierarchical indexing structure. A custom web platform is presented to visualise the structure that is built. The proposed method is tested with two large image collections, up to one million images.},
	language = {English},
	booktitle = {{ESANN} 2016 - 24th {European} {Symposium} on {Artificial} {Neural} {Networks}},
	publisher = {i6doc.com publication},
	author = {Rayar, Frédéric and Barrat, Sabine and Bouali, Fatma and Venturini, Gilles},
	year = {2016},
	note = {Type: Conference paper},
	keywords = {Visualization, Neural networks, Artificial intelligence, Image collections, Learning systems, Large images, Indexing (of information), Digital humanities, Hierarchical indexing, Incremental indexing},
	pages = {659 -- 664},
	file = {Rayar et al. - 2016 - Incremental hierarchical indexing and visualisatio.pdf:C\:\\Users\\Richard\\Zotero\\storage\\RHYH9K3H\\Rayar et al. - 2016 - Incremental hierarchical indexing and visualisatio.pdf:application/pdf},
}

@inproceedings{takale_concept_2016,
	title = {Concept preserving visual summarization of social image search results},
	volume = {21-23-October-2016},
	isbn = {978-1-4503-4808-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996503912&doi=10.1145%2f2998476.2998477&partnerID=40&md5=b08d9f5abbc92d51596a4c16f333875c},
	doi = {10.1145/2998476.2998477},
	abstract = {Existing tag based social media search engines present search results as a ranked list of images. But, they fail to identify visual, textual and geographical concepts present in query results. In this paper, we present an approach for automatic generation of visual, textual and geographical concept preserving summary of social image search results. For user specified query, search results are collected from popular content-sharing websites such as Flickr. Aim of the algorithm is, to generate representative but diverse summary having a set of images, information about locations-of-interest ( LOI) associated with the query, and a set of tags, describing the context of images. The proposed scheme exploits multiple modalities in order to understand context and content of geotagged social images. We formulate the problem as a graph clustering problem, where nodes are images and edge weight is computed as geo-graphical distance, tag-based similarity between images and visual similarity between images. In order to reduce the computational overhead, we implement late fusion of three different edge weight parameters. An innovative Graph based clustering algorithm using Haversine distance formula is proposed for geo-clustering of images. Performance evaluation is based on intrinsic and extrinsic methods. We also present an evaluation protocol having no human intervention for evaluating coverage of geographical spread of images in the final result and cluster coherence. Through empirical study, we demonstrate the effectiveness of our algorithm against state-of-the-art image search result summarization methods. © 2016 ACM.},
	language = {English},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Takale, Sheetal A. and Kulkarni, Prakash J.},
	year = {2016},
	note = {Type: Conference paper},
	keywords = {Clustering algorithms, Graphic methods, Automatic Generation, Computational overheads, Context and content, Evaluation protocol, Graph-based clustering, Multiple modalities, Search engines, Social image searches, Social media searches},
	pages = {21 -- 29},
	file = {Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\6PB6PIQ5\\Takale and Kulkarni - 2016 - Concept Preserving Visual Summarization of Social .pdf:application/pdf},
}

@inproceedings{kaneko_associate-rule-aware_2019,
	title = {An {Associate}-{Rule}-{Aware} {Multidimensional} {Data} {Visualization} {Technique} and {Its} {Application} to {Painting} {Image} {Collections}},
	volume = {2019-July},
	isbn = {978-1-72812-838-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072273613&doi=10.1109%2fIV.2019.00067&partnerID=40&md5=669054b51556139a9d88598c8d8e7e5d},
	doi = {10.1109/IV.2019.00067},
	abstract = {This paper presents a visualization technique for multidimensional datasets containing real and categorical variables. Supposing multidimensional datasets containing real and categorical values, this technique displays a set of axes corresponding to the dimensions of real values. The technique evenly divides the axes into several ranges and displays component bar charts there. It brightly draws the component bar charts if association rules are applied at the corresponding ranges of the dimensions of real values. As a result, this technique highlights association rules so that users can discover important relationships between real and categorical variables in multidimensional datasets. This paper introduces an application of the presented technique to painting image collections. This application visualizes image features and categorical information of painting images and provides a user interface to browse the painting images associated with the multidimensional values. This paper also introduces user evaluation results of the user interfaces for painting image collections. © 2019 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {International} {Conference} on {Information} {Visualisation}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Kaneko, Ayaka and Komatsu, Akiko and Itoh, Takayuki and Wang, Florence Ying},
	editor = {E, Banissi and A, Ursyn and M.W, McK Bannatyne and N, Datia and J.M, Pires and R, Francese and M, Sarfraz and T.G, Wyeld and F, Bouali and G, Venturin and H, Azzag and M, Lebbah and M, Trutschl and U, Cvek and H, Muller and M, Nakayama and S, Kernbach and L, Caruccio and M, Risi and U, Erra and A, Vitiello and V, Rossano},
	year = {2019},
	note = {ISSN: 10939547
Type: Conference paper},
	keywords = {Data visualization, Visualization, Information systems, User interfaces, Image collections, Visualization technique, Association rules, Categorical variables, Image features, ITS applications, Multi-dimensional data visualization, Multi-dimensional datasets, Multidimensional data},
	pages = {358 -- 363},
	file = {Kaneko et al. - 2019 - An Associate-Rule-Aware Multidimensional Data Visu.pdf:C\:\\Users\\Richard\\Zotero\\storage\\RD3T7T5R\\Kaneko et al. - 2019 - An Associate-Rule-Aware Multidimensional Data Visu.pdf:application/pdf},
}

@article{xie_semantic-based_2019,
	title = {A {Semantic}-{Based} {Method} for {Visualizing} {Large} {Image} {Collections}},
	volume = {25},
	issn = {10772626},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046997110&doi=10.1109%2fTVCG.2018.2835485&partnerID=40&md5=f591cf458073eb86ee5dabd71361bdad},
	doi = {10.1109/TVCG.2018.2835485},
	abstract = {Interactive visualization of large image collections is important and useful in many applications, such as personal album management and user profiling on images. However, most prior studies focus on using low-level visual features of images, such as texture and color histogram, to create visualizations without considering the more important semantic information embedded in images. This paper proposes a novel visual analytic system to analyze images in a semantic-aware manner. The system mainly comprises two components: a semantic information extractor and a visual layout generator. The semantic information extractor employs an image captioning technique based on convolutional neural network (CNN) to produce descriptive captions for images, which can be transformed into semantic keywords. The layout generator employs a novel co-embedding model to project images and the associated semantic keywords to the same 2D space. Inspired by the galaxy metaphor, we further turn the projected 2D space to a galaxy visualization of images, in which semantic keywords and images are visually encoded as stars and planets. Our system naturally supports multi-scale visualization and navigation, in which users can immediately see a semantic overview of an image collection and drill down for detailed inspection of a certain group of images. Users can iteratively refine the visual layout by integrating their domain knowledge into the co-embedding process. Two task-based evaluations are conducted to demonstrate the effectiveness of our system. © 1995-2012 IEEE.},
	language = {English},
	number = {7},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Xie, Xiao and Cai, Xiwen and Zhou, Junpei and Cao, Nan and Wu, Yingcai},
	year = {2019},
	pmid = {29993720},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {Data visualization, Visualization, Data mining, Layout, Task analysis, Semantics, Image analysis, Neural networks, literature, embedding, Job analysis, Image captioning, article, Iterative methods, Flow visualization, Semantic information, Image visualization, astronomy, Convolutional Neural Networks (CNN), Galaxies, Interactive visualizations, Layout generators},
	pages = {2362 -- 2377},
	file = {Xie et al. - 2019 - A Semantic-Based Method for Visualizing Large Imag.pdf:C\:\\Users\\Richard\\Zotero\\storage\\BIMQ64YB\\Xie et al. - 2019 - A Semantic-Based Method for Visualizing Large Imag.pdf:application/pdf},
}

@inproceedings{barthel_combining_2022,
	title = {Combining {Semantic} and {Visual} {Image} {Graphs} for {Efficient} {Search} and {Exploration} of {Large} {Dynamic} {Image} {Collections}},
	isbn = {978-1-4503-9497-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139431929&doi=10.1145%2f3552467.3554796&partnerID=40&md5=1d9354adf3cb1eccf639d0afa39ed909},
	doi = {10.1145/3552467.3554796},
	abstract = {Image collections today often consist of millions of images, making it impossible to get an overview of the entire content. In recent years, we have presented several demonstrators for graph-based systems allowing image search and a visual exploration of the collection. Meanwhile, very powerful visual and also joint visual-textual feature vectors have been developed, which are suitable for finding similar images to query images or according to a textual description. A drawback of these image feature vectors is that they have a high number of dimensions, which leads to long search times, especially for large image collections. In this paper, we show how it is possible to significantly reduce the search time even for high-dimensional feature vectors and improve the efficiency of the search system. By combining two different image graphs, on the one hand, an extremely fast approximate nearest neighbor search can be achieved. Experimental results show that the proposed method performs better than state-of-the-art methods. On the other hand, it is possible to visually explore the entire image collection in real time using a standard web browser. Unlike other graph-based search systems, the proposed image graphs can dynamically adapt to the insertion and removal of images from the collection. © 2022 ACM.},
	language = {English},
	booktitle = {{IMuR} 2022 - {Proceedings} of the 2nd {International} {Workshop} on {Interactive} {Multimedia} {Retrieval}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Barthel, Kai Uwe and Hezel, Nico and Schall, Konstantin and Jung, Klaus},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Visualization, Semantics, Image retrieval, Semantic Web, Image collections, Semantic images, Search engines, Nearest neighbor search, Image search, Image graphs, Dynamic images, Features vector, Graph-based, Search system, Search time, Visual image},
	pages = {1 -- 8},
	file = {Barthel et al. - 2022 - Combining Semantic and Visual Image Graphs for Eff.pdf:C\:\\Users\\Richard\\Zotero\\storage\\8YTUJ9Q2\\Barthel et al. - 2022 - Combining Semantic and Visual Image Graphs for Eff.pdf:application/pdf},
}

@inproceedings{gu_igraph_2015,
	title = {{IGraph}: {A} graph-based technique for visual analytics of image and text collections},
	volume = {9397},
	isbn = {978-1-62841-487-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923924430&doi=10.1117%2f12.2074198&partnerID=40&md5=b5186432074bb5b87d6ad6ff5b93e949},
	doi = {10.1117/12.2074198},
	abstract = {In our daily lives, images and texts are among the most commonly found data which we need to handle. We present iGraph, a graph-based approach for visual analytics of large image and text collections. Given such a collection, we compute the similarity between images, the distance between texts, and the connection between image and text to construct iGraph, a compound graph representation which encodes the underlying relationships among these images and texts. To enable effective visual navigation and comprehension of iGraph with tens of thousands of nodes and hundreds of millions of edges, we present a progressive solution that offers collection overview, node comparison, and visual recommendation. Our solution not only allows users to explore the entire collection with representative images and keywords, but also supports detailed comparison for understanding and intuitive guidance for navigation. For performance speedup, multiple GPUs and CPUs are utilized for processing and visualization in parallel. We experiment with two image and text collections and leverage a cluster driving a display wall of nearly 50 million pixels. We show the effectiveness of our approach by demonstrating experimental results and conducting a user study. © 2015 SPIE-IS\&T.},
	language = {English},
	booktitle = {Proceedings of {SPIE} - {The} {International} {Society} for {Optical} {Engineering}},
	publisher = {SPIE},
	author = {Gu, Yi and Wang, Chaoli and Ma, Jun and Nemiroff, Robert J. and Kao, David L.},
	editor = {M.A, Livingston and D.L, Kao and T, Wischgoll and M.C, Hao},
	year = {2015},
	note = {ISSN: 0277786X
Type: Conference paper},
	keywords = {Data visualization, Information analysis, Visual analytics, Visualization, Graph layout, Data handling, Graphic methods, Program processors, Visual Navigation, Compound graphs, Graph-based techniques, Node comparison, Text collection, Visual recommendation},
	file = {Gu et al. - 2015 - IGraph A graph-based technique for visual analyti.pdf:C\:\\Users\\Richard\\Zotero\\storage\\NKVUCK3W\\Gu et al. - 2015 - IGraph A graph-based technique for visual analyti.pdf:application/pdf},
}

@inproceedings{xu_seeing_2015,
	title = {Seeing the big picture from microblogs: {Harnessing} social signals for visual event summarization},
	volume = {2015-January},
	isbn = {978-1-4503-3306-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939630367&doi=10.1145%2f2678025.2701385&partnerID=40&md5=95dd3560f64c682d70aee7061b9ca435},
	doi = {10.1145/2678025.2701385},
	abstract = {We propose an approach to automatically select a set of representative images to generate a concise visual summary of a real-world event from the Tumblr microblogging platform. Central to our approach is a unified graph model with heterogeneous nodes and edges to capture the interrelationship among various entities (e.g., users, posts, images, and tags) in online social media. With the graph representation, we then cast the summarization problem as a graph-based ranking problem by identifying the most representative images regarding to an event. The intuition behind our work is that not only can we crowdsource social media users as sensors to capture and share data, but we can also use them as filters to identify the most useful information through analyzing their interaction in the microblogging network. In addition, we propose a greedy algorithm to encourage diversity among top ranked results for the generation of temporal highlights of targeted events. Our approach is flexible to support different query tasks and is adaptable to additional graph entities and relationships. Copyright © 2015 ACM 978-1-4503-3306-1/15/03 \$15.00.},
	language = {English},
	booktitle = {International {Conference} on {Intelligent} {User} {Interfaces}, {Proceedings} {IUI}},
	publisher = {Association for Computing Machinery},
	author = {Xu, Jiejun and Lu, Tsai-Ching},
	year = {2015},
	note = {Type: Conference paper},
	keywords = {Social networking (online), Graph theory, Crowdsourcing, User interfaces, Graphic methods, Graph representation, Data Sharing, Graph based rankings, Heterogeneous nodes, Micro-blogging platforms, On-line social networks, Online social medias, Tumblr, Visual event summarization},
	pages = {62 -- 66},
	file = {Xu and Lu - 2015 - Seeing the big picture from microblogs Harnessing.pdf:C\:\\Users\\Richard\\Zotero\\storage\\FZY82GTJ\\Xu and Lu - 2015 - Seeing the big picture from microblogs Harnessing.pdf:application/pdf},
}

@inproceedings{zhou_conceptlearner_2015,
	title = {{ConceptLearner}: {Discovering} visual concepts from weakly labeled image collections},
	volume = {07-12-June-2015},
	isbn = {978-1-4673-6964-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959187860&doi=10.1109%2fCVPR.2015.7298756&partnerID=40&md5=18a4ca8d2d5c29dda65cdc7d7ae79b5c},
	doi = {10.1109/CVPR.2015.7298756},
	abstract = {Discovering visual knowledge from weakly labeled data is crucial to scale up computer vision recognition systems, since it is expensive to obtain fully labeled data for a large number of concept categories. In this paper, we propose ConceptLearner, which is a scalable approach to discover visual concepts from weakly labeled image collections. Thousands of visual concept detectors are learned automatically, without human in the loop for additional annotation. We show that these learned detectors could be applied to recognize concepts at image-level and to detect concepts at image region-level accurately. Under domain-specific supervision, we further evaluate the learned concepts for scene recognition on SUN database and for object detection on Pascal VOC 2007. ConceptLearner shows promising performance compared to fully supervised and weakly supervised methods. © 2015 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE Computer Society},
	author = {Zhou, Bolei and Jagadeesh, Vignesh and Piramuthu, Robinson},
	year = {2015},
	note = {ISSN: 10636919
Type: Conference paper},
	keywords = {Computer vision, Pattern recognition, Scene recognition, Concept category, Human-in-the-loop, Learned detectors, Scalable approach, Supervised methods, Vision recognition, Visual knowledge},
	pages = {1492 -- 1500},
	file = {Zhou et al. - 2015 - ConceptLearner Discovering visual concepts from w.pdf:C\:\\Users\\Richard\\Zotero\\storage\\C9785LXY\\Zhou et al. - 2015 - ConceptLearner Discovering visual concepts from w.pdf:application/pdf},
}

@inproceedings{dang-nguyen_multimodal-based_2015,
	title = {Multimodal-based diversified summarization in social image retrieval},
	volume = {1436},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989851922&partnerID=40&md5=f927f1653d02b21755c0be58dbe42594},
	abstract = {In this paper, we describe our approach and its results for the MediaEval 2015 Retrieving Diverse Social Images task. The main strength of the proposed approach is its flexibility that permits to filter out irrelevant images, and to obtain a reliable set of diverse and relevant images. This is done by first clustering similar images according to their textual descriptions and their visual content, and then extracting images from different clusters according to a measure of user's credibility. Experimental results shown that it is stable and has little fluctuation in both single-concept and multi-concept queries.},
	language = {English},
	booktitle = {{CEUR} {Workshop} {Proceedings}},
	publisher = {CEUR-WS},
	author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G. B. and Piras, Luca and Giacinco, Giorgio and Tuveri, Franco and Angioni, Manuela},
	editor = {M, Riegler and R, Sutcliffe and M, Larson and C, Hauff and Y.-H, Yang and B, Ionescu and X, Anguera and M, Soleymani and M, Eskevich and G.J.F, Jones and J, Poignant and M, Sjoberg and S, Papadopoulos},
	year = {2015},
	note = {ISSN: 16130073
Type: Conference paper},
	keywords = {⛔ No DOI found, Image retrieval, Visual content, Multi-modal, Similar image, Social image retrievals, Social images, Textual description},
	file = {Dang-Nguyen et al. - 2015 - Multimodal-based diversified summarization in soci.pdf:C\:\\Users\\Richard\\Zotero\\storage\\WN8URKU4\\Dang-Nguyen et al. - 2015 - Multimodal-based diversified summarization in soci.pdf:application/pdf},
}

@inproceedings{rayar_apod_2016,
	title = {{APoD} {eXplorer}: {Recommendation} system and interactive exploration of a dynamic image collection},
	volume = {2016-August},
	isbn = {978-1-4673-8942-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989933650&doi=10.1109%2fIV.2016.31&partnerID=40&md5=c8045ab548e84ac37afc9eeab15906f1},
	doi = {10.1109/IV.2016.31},
	abstract = {The amount of captured images has increased exponentially these last years. Online available image collections are becoming common thanks to social networks or institutes digitization programs. The context of our work falls into the need to explore such image collections. The literature paradigms are leveraged to meet three constraints: (i) handling medium to large image collections, (ii) handling dynamic image collections and (iii) providing interactive visualisations. In this paper, we describe how our work has been used to build a recommendation system and an interactive exploration platform for dynamic image collection. To illustrate the relevance of such tools, we present APoD eXplorer, an online available platform that enhances the exploration of the NASA Astronomy Picture of the Day image collection. The platform is available at http://frederic.rayar.free.fr/apod/. © 2016 IEEE.},
	language = {English},
	booktitle = {Proceedings of the {International} {Conference} on {Information} {Visualisation}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Rayar, Frédéric and Barrat, Sabine and Bouali, Fatma and Venturini, Gilles},
	editor = {E, Banissi and U, Cvek and C.-C, Lin and F.T, Marchese and C.M, Pun and T.G, Wyeld and M.W.McK, Bannatyne and J, Counsell and M, Trutschl and F, Bouali and R, Burkhard and J, Counsell and A, Ursyn and G, Venturini and G, Grinstein and F, Lin and M.J, Eppler and W.D, Huang and S, Kernbach and M, Sarfraz and J.J, Zhang},
	year = {2016},
	note = {ISSN: 10939547
Type: Conference paper},
	keywords = {Visualization, Recommender systems, Image collections, Large images, Interactive exploration, Dynamic images, Handling dynamics, NASA},
	pages = {118 -- 123},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\BABNREKM\\Rayar et al. - 2016 - APoD eXplorer Recommendation System and Interacti.pdf:application/pdf},
}

@article{phueaksri_towards_2023,
	title = {Towards {Captioning} an {Image} {Collection} from a {Combined} {Scene} {Graph} {Representation} {Approach}},
	volume = {13833 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152572570&doi=10.1007%2f978-3-031-27077-2_14&partnerID=40&md5=5afa4591db04186b2d2490e660588a42},
	doi = {10.1007/978-3-031-27077-2_14},
	abstract = {Most content summarization models from the field of natural language processing summarize the textual contents of a collection of documents or paragraphs. In contrast, summarizing the visual contents of a collection of images has not been researched to this extent. In this paper, we present a framework for summarizing the visual contents of an image collection. The key idea is to collect the scene graphs for all images in the image collection, create a combined representation, and then generate a visually summarizing caption using a scene-graph captioning model. Note that this aims to summarize common contents across all images in a single caption rather than describing each image individually. After aggregating all the scene graphs of an image collection into a single scene graph, we normalize it by using an additional concept generalization component. This component selects the common concept in each sub-graph with ConceptNet based on word embedding techniques. Lastly, we refine the captioning results by replacing a specific noun phrase with a common concept from the concept generalization component to improve the captioning results. We construct a dataset for this task based on the MS-COCO dataset using techniques from image classification and image-caption retrieval. An evaluation of the proposed method on this dataset shows promising performance. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Phueaksri, Itthisak and Kastner, Marc A. and Kawanishi, Yasutomo and Komamizu, Takahiro and Ide, Ichiro},
	editor = {D.-T, Dang-Nguyen and C, Gurrin and A.F, Smeaton and M, Larson and S, Rudinac and M.-S, Dao and C, Trattner and P, Chen},
	year = {2023},
	note = {ISBN: 978-303127076-5
Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Graphic methods, Image collections, Image summarization, Multiple image, Scene-graphs, Image captioning, Natural language processing systems, Classification (of information), Concept generalization, Graph representation, Multiple-image summarization, Scene graph captioning, Visual content},
	pages = {178 -- 190},
	file = {Phueaksri et al. - 2023 - Towards Captioning an Image Collection from a Comb.pdf:C\:\\Users\\Richard\\Zotero\\storage\\NNUPT4X2\\Phueaksri et al. - 2023 - Towards Captioning an Image Collection from a Comb.pdf:application/pdf},
}

@inproceedings{theisen_motif_2023,
	title = {Motif {Mining}: {Finding} and {Summarizing} {Remixed} {Image} {Content}},
	isbn = {978-1-66549-346-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149037259&doi=10.1109%2fWACV56688.2023.00137&partnerID=40&md5=0870775d3d504dfe5fdf8d18231b74a7},
	doi = {10.1109/WACV56688.2023.00137},
	abstract = {On the Internet, images are no longer static; they have become dynamic content. Thanks to the availability of smartphones with cameras and easy-to-use editing software, images can be remixed (i.e., redacted, edited, and re-combined with other content) on-the-fly, allowing a world-wide audience to repeat the process many times. From digital art to memes, the evolution of images through time is now an important topic of study for digital humanists, social scientists, and media forensics specialists. However, because typical data sets in computer vision are composed of static content, there has been limited development of automated algorithms for analyzing remixed content. In this paper, we propose the idea of Motif Mining: the process of finding and summarizing remixed image content in large collections of unlabeled and unsorted data. For the first time, this idea is formalized and a reference implementation grounded in that formalism is introduced. We conduct experiments on three meme-style data sets, including a newly collected set associated with the Russo-Ukrainian conflict. The proposed motif mining approach is able to identify related remixed content that, when compared to similar approaches, more closely aligns with the preferences and expectations of human observers. © 2023 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2023 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision}, {WACV} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Theisen, William and Cedre, Daniel Gonzalez and Carmichael, Zachariah and Moreira, Daniel and Weninger, Tim and Scheirer, Walter},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Data mining, Image processing, Social media, Data set, Image content, Smart phones, Arts computing, Social scientists, Application: social good, Automated algorithms, Digital art, Dynamic content, Internet images},
	pages = {1319 -- 1328},
	file = {Theisen et al. - 2023 - Motif Mining Finding and Summarizing Remixed Imag.pdf:C\:\\Users\\Richard\\Zotero\\storage\\FR7T3NNI\\Theisen et al. - 2023 - Motif Mining Finding and Summarizing Remixed Imag.pdf:application/pdf},
}

@inproceedings{barthel_graph_2017,
	title = {Graph navigation for exploring very large image collections},
	volume = {5},
	isbn = {978-989-758-226-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047844977&doi=10.5220%2f0006274804110416&partnerID=40&md5=a23bdfdf0a08df05215f6cbaa32eb1f8},
	doi = {10.5220/0006274804110416},
	abstract = {We present a new approach to visually browse very large sets of untagged images. In this paper we describe how to generate high quality image descriptors/features using transformed activations of a convolutional neural network. These features are used to model image similarities, which again are used to build a hierarchical image graph. We show how such an image graph can be constructed efficiently. After investigating several browsing and visualization concepts, we found best user experience and ease of usage is achieved by projecting sub-graphs onto a regular 2D-image map. This allows users to explore the image graph similar to navigation services. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
	language = {English},
	booktitle = {{VISIGRAPP} 2017 - {Proceedings} of the 12th {International} {Joint} {Conference} on {Computer} {Vision}, {Imaging} and {Computer} {Graphics} {Theory} and {Applications}},
	publisher = {SciTePress},
	author = {Barthel, Kai Uwe and Hezel, Nico},
	editor = {F, Imai and A, Tremeau and J, Braz},
	year = {2017},
	note = {Type: Conference paper},
	keywords = {Navigation, Visualization, Computer vision, Convolution, Neural networks, Computer graphics, User experience, Convolutional neural network, Flow visualization, Image graphs, Natural resources exploration, Browsing, CBIR, Graph navigation, High quality images, Navigation service},
	pages = {411 -- 416},
	file = {Barthel and Hezel - 2017 - Graph navigation for exploring very large image co.pdf:C\:\\Users\\Richard\\Zotero\\storage\\KJIMMYX5\\Barthel and Hezel - 2017 - Graph navigation for exploring very large image co.pdf:application/pdf},
}

@article{celikkale_generating_2021,
	title = {Generating visual story graphs with application to photo album summarization},
	volume = {90},
	issn = {09235965},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092890679&doi=10.1016%2fj.image.2020.116033&partnerID=40&md5=39504f6af43037cedf828ac1433b3d41},
	doi = {10.1016/j.image.2020.116033},
	abstract = {Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art. © 2020},
	language = {English},
	journal = {Signal Processing: Image Communication},
	author = {Celikkale, Bora and Erdogan, Goksu and Erdem, Aykut and Erdem, Erkut},
	year = {2021},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Signal processing, Directed graphs, User study, Visual data, Photo collections, State of the art, Photo album, Experimental analysis, Image communication systems, Image prediction, Spatio temporal features},
	file = {Celikkale et al. - 2021 - Generating visual story graphs with application to.pdf:C\:\\Users\\Richard\\Zotero\\storage\\D6U5QYHP\\Celikkale et al. - 2021 - Generating visual story graphs with application to.pdf:application/pdf},
}

@inproceedings{batko_clan_2014,
	title = {{CLAN} photo presenter: {Multi}-modal summarization tool for image collections},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899747928&doi=10.1145%2f2578726.2582623&partnerID=40&md5=b4f235ebc4e6335e185a66b672c3bf9e},
	doi = {10.1145/2578726.2582623},
	abstract = {Effective management of multimedia data is becoming vital for success in the modern era of omnipresent data. Summarization tools, which allow users to quickly get the gist of a given data collection and have proven their usefulness in text domain, are now gaining popularity also in multimedia processing. However, existing algorithms provide visual-only summaries for image collections, which are difficult to index and search. This paper introduces a prototype software tool that automatically creates multi-modal summaries of personal image collections by enriching the visual collage with keyword annotation. The result is presented as a web page that allows users to browse and share the summarized data. Copyright is held by the owner/author(s).},
	language = {English},
	booktitle = {{ICMR} 2014 - {Proceedings} of the {ACM} {International} {Conference} on {Multimedia} {Retrieval} 2014},
	publisher = {Association for Computing Machinery},
	author = {Batko, Michal and Budikova, Petra and Elias, Petr and Zezula, Pavel},
	year = {2014},
	note = {Type: Conference paper},
	keywords = {Tools, Clustering, Information management, Image collections, Multi-modal, Automatic annotation, Content-based},
	pages = {541 -- 542},
	file = {Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\FV7FZYYQ\\Batko et al. - 2014 - CLAN Photo Presenter Multi-modal Summarization To.pdf:application/pdf},
}

@article{schaefer_interactive_2015,
	title = {Interactive browsing of image collections on mobile devices},
	volume = {74},
	issn = {13807501},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940567203&doi=10.1007%2fs11042-014-1875-8&partnerID=40&md5=e98d21b96d9be365bf048f921b9d0c85},
	doi = {10.1007/s11042-014-1875-8},
	abstract = {Image collections are growing at a rapid rate and hence visual information is becoming more and more important. Clearly, these image repositories need to be managed, and tools for effectively and efficiently searching image databases are highly sought after, especially on mobile devices where more and more images are being stored. In this paper, we present an image browsing system for interactive exploration of image collections on mobile devices. Images are arranged so that visually similar images are grouped together while large image repositories become accessible through a hierarchical, browsable tree structure, arranged on a hexagonal lattice. The developed system provides an intuitive and fast interface for navigating through image databases using a variety of touch gestures. © 2014, Springer Science+Business Media New York.},
	language = {English},
	number = {19},
	journal = {Multimedia Tools and Applications},
	author = {Schaefer, Gerald and Tallyn, Matthew and Felton, Daniel and Plant, William and Edmundson, David},
	year = {2015},
	note = {Publisher: Kluwer Academic Publishers
Type: Article},
	keywords = {Database systems, Mobile computing, Image collections, Visual information, Trees (mathematics), Interactive exploration, Hexagonal lattice, Image database, Image repository, Interactive browsing, Mobile devices, Mobile image browsing},
	pages = {8267 -- 8277},
	file = {Schaefer et al. - 2015 - Interactive browsing of image collections on mobil.pdf:C\:\\Users\\Richard\\Zotero\\storage\\Y5JGIIBA\\Schaefer et al. - 2015 - Interactive browsing of image collections on mobil.pdf:application/pdf},
}

@article{rudinac_learning_2013,
	title = {Learning crowdsourced user preferences for visual summarization of image collections},
	volume = {15},
	issn = {15209210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884543863&doi=10.1109%2fTMM.2013.2261481&partnerID=40&md5=aa51df7c191a4d5bcaf7503f9a897b01},
	doi = {10.1109/TMM.2013.2261481},
	abstract = {In this paper we propose a novel approach to selecting images suitable for inclusion in the visual summaries. The approach is grounded in insights about how people summarize image collections. We utilize the Amazon Mechanical Turk crowdsourcing platform to obtain a large number of manually created visual summaries as well as information about criteria for image inclusion in the summary. Based on these large-scale user tests, we propose an automatic image selection approach, which jointly utilizes the analysis of image content, context, popularity, visual aesthetic appeal as well as the sentiment derived from the comments posted on the images. In our approach we do not describe images based on their properties only, but also in the context of semantically related images, which improves robustness and effectively enables propagation of sentiment, aesthetic appeal as well as various inherent attributes associated with a particular group of images. We discuss the phenomenon of a low inter-user agreement, which makes an automatic evaluation of visual summaries a challenging task and propose a solution inspired by the text summarization and machine translation communities. The experiments performed on a collection of geo-referenced Flickr images demonstrate the effectiveness of our image selection approach. © 2013 IEEE.},
	language = {English},
	number = {6},
	journal = {IEEE Transactions on Multimedia},
	author = {Rudinac, Stevan and Larson, Martha and Hanjalic, Alan},
	year = {2013},
	note = {Type: Article},
	keywords = {Crowdsourcing, Signal processing, Social media, Image content, Image Aesthetics, Image sets, Learning to rank, Multimedia systems, Sentiment analysis, user-informed visual summarization},
	pages = {1231 -- 1243},
	file = {Rudinac et al. - 2013 - Learning crowdsourced user preferences for visual .pdf:C\:\\Users\\Richard\\Zotero\\storage\\QIE2ZGGM\\Rudinac et al. - 2013 - Learning crowdsourced user preferences for visual .pdf:application/pdf},
}

@inproceedings{sharma_novel_2013,
	title = {A novel variety-based {3DTV} content generation scheme for casually captured sparse photo collections},
	isbn = {978-1-4799-1369-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898885676&doi=10.1109%2f3DTV.2013.6676649&partnerID=40&md5=dbccdd5b7386d496149265532e80a51a},
	doi = {10.1109/3DTV.2013.6676649},
	abstract = {This paper presents a novel parameterized variety-based 3D exploration model to comprehend the sparse unstructured collection of photographs, and automatically plan virtual 3D tours of the world's landmarks through interesting viewpoints without explicit 3D reconstruction. The proposed system analyzes the collection of unstructured but related image data containing the same location or environment to create a parameterized scene graph: a data structure that conveys spatial relations and enable smooth virtual navigation between photos. A novel statistical-heuristic criteria is evolved exploiting the scene spatial layout and appearance to automatically identify best available portals between photographs. Once well connected, the graph is parameterized and consistently rendered choosing visually compelling 3D transition paths, maintaining a pleasing essence of parallax. The system's ability is demonstrated on several casually captured personal photo collections of heritage sites and imagery gathered from 'Flickr' data. © 2013 IEEE.},
	language = {English},
	booktitle = {{3DTV}-{Conference}},
	publisher = {IEEE Computer Society},
	author = {Sharma, Mansi and Chaudhury, Santanu and Lall, Brejesh},
	year = {2013},
	note = {ISSN: 21612021
Type: Conference paper},
	keywords = {Photography, Photo collections, Scene graph, Digital television, Full-perspective image variety, Geometrical optics, Parameterization, Personal photo collection, Spatial relations, Three dimensional, View synthesis, Virtual 3D photo tours, Virtual navigation},
	file = {Sharma et al. - 2013 - A novel variety-based 3DTV content generation sche.pdf:C\:\\Users\\Richard\\Zotero\\storage\\3MZPMY65\\Sharma et al. - 2013 - A novel variety-based 3DTV content generation sche.pdf:application/pdf},
}

@article{camargo_kernel-based_2013,
	title = {A kernel-based framework for image collection exploration},
	volume = {24},
	issn = {1045926X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870864463&doi=10.1016%2fj.jvlc.2012.10.008&partnerID=40&md5=ba70e270d695f2040ced9389f44a55c6},
	doi = {10.1016/j.jvlc.2012.10.008},
	abstract = {While search engines have been a successful tool to search text information, image search systems still face challenges. The keyword-based query paradigm used to search in image collection systems, which has been successful in text retrieval, may not be useful in scenarios where the user does not have the precise way to express a visual query. Image collection exploration is a new paradigm where users interact with the image collection to discover useful and relevant pictures. This paper proposes a framework for the construction of an image collection exploration system based on kernel methods, which offers a mathematically strong basis to address each stage of an image collection exploration system: image representation, summarization, visualization and interaction. In particular, our approach emphasizes a semantic representation of images using kernel functions, which can be seamlessly harnessed across all system components. Experiments were conducted with real users to verify the effectiveness and efficiency of the proposed strategy. © 2012 Elsevier Ltd.},
	language = {English},
	number = {1},
	journal = {Journal of Visual Languages and Computing},
	author = {Camargo, Jorge E. and Caicedo, Juan C. and Gonzalez, Fabio A.},
	year = {2013},
	note = {Type: Article},
	pages = {53 -- 67},
	file = {Camargo et al. - 2013 - A kernel-based framework for image collection expl.pdf:C\:\\Users\\Richard\\Zotero\\storage\\HZ56HXSW\\Camargo et al. - 2013 - A kernel-based framework for image collection expl.pdf:application/pdf},
}

@article{rudinac_generating_2013,
	title = {Generating visual summaries of geographic areas using community-contributed images},
	volume = {15},
	issn = {15209210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877905242&doi=10.1109%2fTMM.2013.2237896&partnerID=40&md5=7c21e36af60ca63a03b29d8eaa97ddb7},
	doi = {10.1109/TMM.2013.2237896},
	abstract = {In this paper, we present a novel approach for automatic visual summarization of a geographic area that exploits user-contributed images and related explicit and implicit metadata collected from popular content-sharing websites. By means of this approach, we search for a limited number of representative but diverse images to represent the area within a certain radius around a specific location. Our approach is based on the random walk with restarts over a graph that models relations between images, visual features extracted from them, associated text, as well as the information on the uploader and commentators. In addition to introducing a novel edge weighting mechanism, we propose in this paper a simple but effective scheme for selecting the most representative and diverse set of images based on the information derived from the graph. We also present a novel evaluation protocol, which does not require input of human annotators, but only exploits the geographical coordinates accompanying the images in order to reflect conditions on image sets that must necessarily be fulfilled in order for users to find them representative and diverse. Experiments performed on a collection of Flickr images, captured around 207 locations in Paris, demonstrate the effectiveness of our approach. © 1999-2012 IEEE.},
	language = {English},
	number = {4},
	journal = {IEEE Transactions on Multimedia},
	author = {Rudinac, Stevan and Hanjalic, Alan and Larson, Martha},
	year = {2013},
	note = {Type: Article},
	keywords = {Metadata, Signal processing, Social media, Image sets, Multimedia systems, Automatic evaluation, Geographic areas, Graph-based models, Multi-modal fusion},
	pages = {921 -- 932},
	file = {Rudinac et al. - 2013 - Generating visual summaries of geographic areas us.pdf:C\:\\Users\\Richard\\Zotero\\storage\\V3J57EM6\\Rudinac et al. - 2013 - Generating visual summaries of geographic areas us.pdf:application/pdf},
}

@inproceedings{schaefer_interactive_2013,
	title = {Interactive exploration of large photo libraries},
	isbn = {978-1-4503-2119-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899501541&doi=10.1145%2f2556871.2556873&partnerID=40&md5=5cfa258696935ca0b8ca5ad8d796007f},
	doi = {10.1145/2556871.2556873},
	abstract = {Visual information, in particular in form of images, is becoming increasingly important, and consequently efficient and effective tools for managing these rapidly growing collections are highly sought after. Interactive image database browsing systems provide an interesting alternative to retrievalbased approaches as they let the user explore an image dataset in an intuitive fashion. Based on content-based concepts, large image collections are visualised so that visually similar images are located close to each other in the visualisation space. Once displayed, the user can then interactively browse through the image collection. The main approaches to visualising and browsing image collections are mapping-based techniques, which are based on dimensionality reduction, and clustering-based methods, that group similar images together. In this paper, we highlight how these two approaches can be effectively combined to devise an intuitive image database navigation system that has low computational requirements, both offline and online, and organises images based on colour content on a spherical visualisation space while providing hierarchical access to large image datasets. © 2013 ACM.},
	language = {English},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Schaefer, Gerald},
	year = {2013},
	note = {Type: Conference paper},
	keywords = {Visualization, Dimensionality reduction, Cloud computing, Visual information, Interactive exploration, Image browsing, Interactive images, Image database, Computational requirements, Image database navigations, Navigation systems},
	pages = {3 -- 5},
	file = {Schaefer - 2013 - Interactive exploration of large photo libraries.pdf:C\:\\Users\\Richard\\Zotero\\storage\\EGQ848JT\\Schaefer - 2013 - Interactive exploration of large photo libraries.pdf:application/pdf},
}

@inproceedings{camargo_mics_2013,
	title = {{MICS}: {Multimodal} image collection summarization by optimal reconstruction subset selection},
	isbn = {978-1-4799-1056-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890815931&doi=10.1109%2fColombianCC.2013.6637539&partnerID=40&md5=ac8ff8d3c91e22af66955af5aaaf422a},
	doi = {10.1109/ColombianCC.2013.6637539},
	abstract = {This paper presents a new method to automatically select a set of representative images from a larger set of retrieved images for a given query. We define an image collection summary as a subset of images from the collection, which are visually and semantically representative. To build such a summary we propose MICS, a method that fuses two modalities, textual and visual, in a common latent space, and use it to find a subset of images from which the collection visual content could be reconstructed. We conducted experiments on a collection of tagged images and demonstrate the ability of our approach to build summaries with representative visual and semantic content. The initial results show that the proposed method is able to build a meaningful summary that can be integrated in an image collection exploration system. © 2013 IEEE.},
	language = {English},
	booktitle = {2013 8th {Computing} {Colombian} {Conference}, {8CCC} 2013},
	publisher = {IEEE Computer Society},
	author = {Camargo, Jorge E. and Gonzalez, Fabio A.},
	year = {2013},
	note = {Type: Conference paper},
	keywords = {Computer science, Semantics, Information retrieval, Computer programming, Image collections, Visual content, Learning systems, Multi-modal, Multi-modal image, Latent factor analysis, Retrieved images, Semantic content, Subset selection},
	file = {Camargo and Gonzalez - 2013 - MICS Multimodal image collection summarization by.pdf:C\:\\Users\\Richard\\Zotero\\storage\\C4STBLQD\\Camargo and Gonzalez - 2013 - MICS Multimodal image collection summarization by.pdf:application/pdf},
}

@article{li_visualization_2019,
	title = {Visualization of {Photo} {Album}: {Selecting} a {Representative} {Photo} of a {Specific} {Event}},
	volume = {11448 LNCS},
	issn = {03029743},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065442992&doi=10.1007%2f978-3-030-18590-9_9&partnerID=40&md5=55d338c3e89515eadcff9bcba9236b12},
	doi = {10.1007/978-3-030-18590-9_9},
	abstract = {In order to effectively manage photos in personal photo album and improve the efficiency of re-finding photos, the visualization of photo album has received attention. The most popular and reasonable visualization method is to display a representative photo of each photo cluster. We studied the characteristics of representative photos and then proposed a method of selecting the representative photos from a set of photos related to a specific event. The method mainly considered two aspects of photos: aesthetic quality and memorable factor. Aesthetic quality contains the area and location of the salient region and the sharpness of photo; memorable factors contain the salient people and text information. The experimental data sets are real-world personal photo collections, including more than 7,000 photos and more than 2000 specific events. The experimental results show the efficiency and reliability of selecting representative photos to visualization of photo album. © 2019, Springer Nature Switzerland AG.},
	language = {English},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Li, Yukun and Geng, Ming and Liu, Fenglian and Zhang, Degan},
	editor = {J, Natwichai and G, Li and Y, Tong and J, Yang and J, Gama},
	year = {2019},
	note = {ISBN: 978-303018589-3
Publisher: Springer Verlag
Type: Conference paper},
	keywords = {Visualization, Database systems, Image enhancement, Efficiency, Flow visualization, Photo album, Personal photo collection, Aesthetic qualities, Efficiency and reliability, Representative photo, Salient regions, Text information, Visualization method},
	pages = {128 -- 141},
	file = {Li et al. - 2019 - Visualization of Photo Album Selecting a Represen.pdf:C\:\\Users\\Richard\\Zotero\\storage\\AKDSDF4U\\Li et al. - 2019 - Visualization of Photo Album Selecting a Represen.pdf:application/pdf},
}

@article{van_der_corput_comparing_2017,
	title = {Comparing {Personal} {Image} {Collections} with {PICTuReVis}},
	volume = {36},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13188},
	doi = {10.1111/cgf.13188},
	abstract = {Digital image collections contain a wealth of information, which for instance can be used to trace illegal activities and investigate criminal networks. We present a method that enables analysts to reveal relations among people, based on the patterns in their collections. Similar temporal and spatial patterns can be found using a parameterized algorithm, visualization is used to choose the right parameters and to inspect the patterns found. The visualization shows relations between image properties: the person it belongs to, the concepts in the image, its time stamp and location. We demonstrate the method with image collections of 10, 000 people containing 460, 000 images in total.},
	language = {en},
	number = {3},
	urldate = {2024-01-22},
	journal = {Computer Graphics Forum},
	author = {Van Der Corput, Paul and Van Wijk, Jarke J.},
	month = jun,
	year = {2017},
	pages = {295--304},
	file = {Van Der Corput and Van Wijk - 2017 - Comparing Personal Image Collections with PICTuReV.pdf:C\:\\Users\\Richard\\Zotero\\storage\\DGB5ESLI\\Van Der Corput and Van Wijk - 2017 - Comparing Personal Image Collections with PICTuReV.pdf:application/pdf},
}

@article{radiano_story_2018,
	title = {Story {Albums}: {Creating} {Fictional} {Stories} {From} {Personal} {Photograph} {Sets}},
	volume = {37},
	issn = {0167-7055, 1467-8659},
	shorttitle = {Story {Albums}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cgf.13099},
	doi = {10.1111/cgf.13099},
	abstract = {We present a method for the automatic creation of ﬁctional storybooks based on personal photographs. Unlike previous attempts that summarize such collections by picking salient or diverse photos, or creating personal literal narratives, we focus on the creation of ﬁctional stories. This provides new value to users, as well as an engaging way for people (especially children) to experience their own photographs. We use a graph model to represent an artist-generated story, where each node is a ‘frame’, akin to frames in comics or storyboards. A node is described by story elements, comprising actors, location, supporting objects and time. The edges in the graph encode connections between these elements and provide the discourse of the story. Based on this construction, we develop a constraint satisfaction algorithm for one-to-one assignment of nodes to photographs. Once each node is assigned to a photograph, a visual depiction of the story can be generated in different styles using various templates. We show results of several ﬁctional visual stories created from different personal photo sets and in different styles.},
	language = {en},
	number = {1},
	urldate = {2024-01-22},
	journal = {Computer Graphics Forum},
	author = {Radiano, O. and Graber, Y. and Mahler, M. and Sigal, L. and Shamir, A.},
	month = feb,
	year = {2018},
	pages = {19--31},
	file = {Radiano et al. - 2018 - Story Albums Creating Fictional Stories From Pers.pdf:C\:\\Users\\Richard\\Zotero\\storage\\QMSBUVE8\\Radiano et al. - 2018 - Story Albums Creating Fictional Stories From Pers.pdf:application/pdf},
}

@article{zheng_album_2014,
	title = {Album {Quickview} in {Comic}-like {Layout} via {Quartet} {Analysis}},
	url = {http://diglib.eg.org/handle/10.2312/pgs.20141253.061-066},
	doi = {10.2312/PGS.20141253},
	abstract = {For clear summary and efﬁcient search of images for album, which carries a story of life record, we propose a new approach for quickview of album in comic-like layout via quartet analysis. How to organize the images in album and in what way to display images in collage are two key problems for album quickview. For the ﬁrst problem, we take the idea of model organization method based on quartet analysis to construct categorization tree to organize the images; while for the second problem, we utilize the topological structure of categorization tree to decompose it into multiple groups of images and extract representative image from each group for subsequent collage. For the collage part, we choose comic-like layout to present collage because comic provides a concise way for storytelling and it has variablitiy in layout styles, which is suitable for album summary. Experiments demonstrate that our method could organize the images effectively and present images in collage with diverse styles as well.},
	language = {en},
	urldate = {2024-01-22},
	journal = {Pacific Graphics Short Papers},
	author = {Zheng, Zhibin and Zhang, Yan and Miao, Zheng and Sun, Zhengxing},
	year = {2014},
	note = {Artwork Size: 6 pages
ISBN: 9783905674736
Publisher: The Eurographics Association},
	keywords = {General, I.4.0 [Computer Processing and Computer Vision], Image displays},
	pages = {6 pages},
	file = {Zheng et al. - 2014 - Album Quickview in Comic-like Layout via Quartet A.pdf:C\:\\Users\\Richard\\Zotero\\storage\\GARIK44R\\Zheng et al. - 2014 - Album Quickview in Comic-like Layout via Quartet A.pdf:application/pdf},
}

@article{fang_visualizing_2013,
	title = {Visualizing {Natural} {Image} {Statistics}},
	volume = {19},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/6361387},
	doi = {10.1109/TVCG.2012.312},
	abstract = {Natural image statistics is an important area of research in cognitive sciences and computer vision. Visualization of statistical results can help identify clusters and anomalies as well as analyze deviation, distribution, and correlation. Furthermore, they can provide visual abstractions and symbolism for categorized data. In this paper, we begin our study of visualization of image statistics by considering visual representations of power spectra, which are commonly used to visualize different categories of images. We show that they convey a limited amount of statistical information about image categories and their support for analytical tasks is ineffective. We then introduce several new visual representations, which convey different or more information about image statistics. We apply ANOVA to the image statistics to help select statistically more meaningful measurements in our design process. A task-based user evaluation was carried out to compare the new visual representations with the conventional power spectra plots. Based on the results of the evaluation, we made further improvement of visualizations by introducing composite visual representations of image statistics.},
	number = {7},
	urldate = {2024-01-22},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fang, Hui and Tam, Gary Kwok-Leung and Borgo, Rita and Aubrey, Andrew J. and Grant, Philip W. and Rosin, Paul L. and Wallraven, Christian and Cunningham, Douglas and Marshall, David and Chen, Min},
	month = jul,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1228--1241},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Richard\\Zotero\\storage\\4VPZ4F2W\\6361387.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\48NV6LEQ\\Fang et al. - 2013 - Visualizing Natural Image Statistics.pdf:application/pdf},
}

@article{wu_image_2023,
	title = {Image {Collage} on {Arbitrary} {Shape} via {Shape}-{Aware} {Slicing} and {Optimization}},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/10081386},
	doi = {10.1109/TVCG.2023.3262039},
	abstract = {Image collage is a very useful tool for visualizing an image collection. Most of the existing methods and commercial applications for generating image collages are designed on simple shapes, such as rectangular and circular layouts. This greatly limits the use of image collages in some artistic and creative settings. Although there are some methods that can generate irregularly-shaped image collages, they often suffer from severe image overlapping and excessive blank space. This prevents such methods from being effective information communication tools. In this paper, we present a shape slicing algorithm and an optimization scheme that can create image collages of arbitrary shapes in an informative and visually pleasing manner given an input shape and an image collection. To overcome the challenge of irregular shapes, we propose a novel algorithm, called Shape-Aware Slicing, which partitions the input shape into cells based on medial axis and binary slicing tree. Shape-Aware Slicing, which is designed specifically for irregular shapes, takes human perception and shape structure into account to generate visually pleasing partitions. Then, the layout is optimized by analyzing input images with the goal of maximizing the total salient regions of the images. To evaluate our method, we conduct extensive experiments and compare our results against previous work. The evaluations show that our proposed algorithm can efficiently arrange image collections on irregular shapes and create visually superior results than prior work and existing commercial tools.},
	urldate = {2024-01-22},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wu, Dong-Yi and Le, Thi-Ngoc-Hanh and Yao, Sheng-Yi and Lin, Yun-Chen and Lee, Tong-Yee},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--13},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Richard\\Zotero\\storage\\HRZSRGAZ\\10081386.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Richard\\Zotero\\storage\\8GXSJHZP\\Wu et al. - 2023 - Image Collage on Arbitrary Shape via Shape-Aware S.pdf:application/pdf},
}
